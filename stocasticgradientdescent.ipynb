{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: torchaudio in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: transformers in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: datasets in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: filelock in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install torch torchaudio transformers datasets scikit-learn numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')\n",
    "audio_embeddings = torch.load('audio_embeddings.pt', map_location=device)\n",
    "\n",
    "print(len(audio_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "audio_dataset = load_dataset(\n",
    "    'TrainingDataPro/speech-emotion-recognition-dataset',\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "def convert_to_target(emotion):\n",
    "    if emotion == 'euphoric':\n",
    "        return 1\n",
    "    elif emotion == 'joyfully':\n",
    "        return 2\n",
    "    elif emotion == 'sad':\n",
    "        return 3\n",
    "    elif emotion == 'surprised':\n",
    "        return 4\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "emotions = ['euphoric', 'joyfully', 'sad', 'surprised']\n",
    "targets = []\n",
    "\n",
    "for emotion in emotions:\n",
    "    for sample in audio_dataset[emotion]:\n",
    "        targets.append(convert_to_target(emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 512)\n",
      "[[-0.0197334  -0.00249679  0.11328787 ... -0.08363502  0.04762014\n",
      "   0.08813638]\n",
      " [-0.01700936 -0.00169819  0.12584828 ... -0.08392757  0.05042014\n",
      "   0.0900396 ]\n",
      " [-0.01665157 -0.00179881  0.11708343 ... -0.0858629   0.04423774\n",
      "   0.08991268]\n",
      " ...\n",
      " [-0.01731364 -0.00215796  0.11473525 ... -0.0838877   0.04435505\n",
      "   0.08995587]\n",
      " [-0.01807118 -0.00259435  0.11262784 ... -0.08339463  0.04653695\n",
      "   0.08947439]\n",
      " [-0.01466641 -0.00274683  0.1172166  ... -0.08552615  0.04621886\n",
      "   0.09083286]]\n"
     ]
    }
   ],
   "source": [
    "# Flatten embeddings\n",
    "flattened_embeddings = [embedding.view(-1) for embedding in audio_embeddings]\n",
    "embedding_matrix = torch.stack(flattened_embeddings)\n",
    "\n",
    "# Convert embeddings from tensor to numpy arrays\n",
    "embedding_matrix = embedding_matrix.detach().cpu().numpy()\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAINING = 15\n",
    "\n",
    "def create_splits(data, targets):\n",
    "    train_data = []\n",
    "    train_targets = []\n",
    "    test_data = []\n",
    "    test_targets = []\n",
    "    \n",
    "    for index in range(80):\n",
    "        if index % 20 < NUM_TRAINING:\n",
    "            train_data.append(data[index])\n",
    "            train_targets.append(targets[index])\n",
    "        else:\n",
    "            test_data.append(data[index])\n",
    "            test_targets.append(targets[index])\n",
    "    \n",
    "    return train_data, test_data, train_targets, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test data sets\n",
    "X_train, X_test, y_train, y_test = create_splits(embedding_matrix, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2160 candidates, totalling 10800 fits\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.1s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=constant, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.2s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=optimal, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=log, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=modified_huber, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=squared_hinge, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=invscaling, loss=squared_hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=500, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=perceptron, max_iter=2000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=l1; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, loss=modified_huber, max_iter=2000, penalty=l2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "2160 fits failed out of a total of 10800.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "328 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'modified_huber', 'squared_hinge', 'epsilon_insensitive', 'hinge', 'perceptron', 'huber', 'squared_error', 'squared_epsilon_insensitive'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "160 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'log_loss', 'modified_huber', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'huber', 'squared_hinge', 'perceptron', 'hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "243 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'squared_error', 'squared_hinge', 'hinge', 'log_loss', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'huber', 'modified_huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "263 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'modified_huber', 'squared_epsilon_insensitive', 'squared_hinge', 'log_loss', 'epsilon_insensitive', 'hinge', 'huber', 'perceptron'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "174 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'modified_huber', 'huber', 'squared_epsilon_insensitive', 'squared_error', 'epsilon_insensitive', 'squared_hinge', 'log_loss', 'hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "265 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'squared_error', 'squared_epsilon_insensitive', 'modified_huber', 'epsilon_insensitive', 'squared_hinge', 'log_loss', 'huber', 'hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "249 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'hinge', 'squared_epsilon_insensitive', 'perceptron', 'modified_huber', 'log_loss', 'huber', 'squared_hinge', 'epsilon_insensitive'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "205 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'perceptron', 'huber', 'hinge', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'modified_huber', 'squared_error', 'log_loss'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "111 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'squared_error', 'modified_huber', 'log_loss', 'hinge', 'huber', 'perceptron', 'epsilon_insensitive', 'squared_hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "162 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'perceptron', 'squared_error', 'modified_huber', 'squared_epsilon_insensitive', 'epsilon_insensitive', 'hinge', 'squared_hinge', 'huber'}. Got 'log' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/diptobiswas/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.14307692 0.1        0.1        ... 0.22888889 0.1        0.1       ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize the SGD Classifier\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],  # Loss function to be used\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],  # The penalty (aka regularization term) to be used\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],  # Constant that multiplies the regularization term\n",
    "    'max_iter': [500, 1000, 2000],  # Maximum number of iterations over the training data\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],  # Learning rate schedule\n",
    "    'eta0': [0.1, 0.01, 0.001]  # Initial learning rate (for constant or invscaling schedules)\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object with SGDClassifier as the estimator\n",
    "grid_search = GridSearchCV(sgd, param_grid, cv=5, scoring='f1_weighted', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator (SGD model with best-found parameters)\n",
    "model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Define the F1 scorer\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Run cross-validation with both accuracy and F1 score\n",
    "cv_results = cross_validate(\n",
    "    model,\n",
    "    embedding_matrix,\n",
    "    targets,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring={'accuracy': 'accuracy', 'f1_weighted': f1_scorer},\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Extract and print the results\n",
    "train_accuracy = cv_results['train_accuracy']\n",
    "test_accuracy = cv_results['test_accuracy']\n",
    "train_f1 = cv_results['train_f1_weighted']\n",
    "test_f1 = cv_results['test_f1_weighted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKYAAAJOCAYAAACN2Q8zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAADgl0lEQVR4nOzdd1hTZxsG8DsJe4koCiqCew/EvTdq3XVrRevo12pdtba21tVa27pb22prFffCWTdu3HvvhQtxi4is5Hx/vD2BsEfgJHD/rutchJOTkychJ+Q8eZ/nVUmSJIGIiIiIiIiIiCibqZUOgIiIiIiIiIiIcicmpoiIiIiIiIiISBFMTBERERERERERkSKYmCIiIiIiIiIiIkUwMUVERERERERERIpgYoqIiIiIiIiIiBTBxBQRERERERERESmCiSkiIiIiIiIiIlIEE1NERERERERERKQIJqaIiMgkqVQqTJw4Md23u3fvHlQqFfz9/Y0eExGZj4kTJ0KlUqVp24y+35Dg7+8PlUqFe/fu6dc1btwYjRs3TvW2+/fvh0qlwv79+40aE/+mRETmg4kpIiJKlnyyoVKpcOjQoUTXS5IEDw8PqFQqtG3bVoEIM04+GUpq6dGjR6q39/LygkqlQvPmzZO8/u+//9bv79SpU8YO3+xER0djzpw58Pb2hpOTE5ydnVGhQgUMHjwY165dUzq8DLt69SpUKhVsbGzw+vVrpcPJEiEhIfj666/RpEkTODo6pppEOHLkCOrXrw87Ozu4ublh2LBhCA8PT9N9ycmkpJZ58+YZ6RFlTlhYGCZNmoQqVarAwcEBtra2qFixIr766is8fvxY6fBSFBMTg/z586N+/frJbiO/r1erVi0bI8uYbdu2MflERJQDWCgdABERmT4bGxusWLEi0cnMgQMH8PDhQ1hbWysUWeYNGzYMNWrUMFjn5eWVptva2Nhg3759ePLkCdzc3AyuW758OWxsbBAZGWmsUM3ahx9+iO3bt6Nnz54YNGgQYmJicO3aNWzZsgV169ZF2bJllQ4xQ5YtWwY3Nze8evUKAQEBGDhwoNIhGd3169fx888/o1SpUqhUqRKOHj2a7Lbnzp1Ds2bNUK5cOcycORMPHz7E9OnTcfPmTWzfvj3N9/nnn3/CwcHBYF2tWrUy/BiM5c6dO2jevDnu37+Prl27YvDgwbCyssKFCxfwzz//YMOGDbhx44bSYSbL0tISXbt2xfz58xEcHAxPT89E2xw8eBAPHz7EyJEjM3Vfu3btytTt02Lbtm34/fffk0xOvX//HhYWPNUhIjIHfLcmIqJUtWnTBmvXrsWvv/5q8EF/xYoV8PHxwfPnzxWMLnMaNGiALl26ZOi29erVw8mTJ7F69WoMHz5cv/7hw4cICgpCp06dsG7dOmOFqgidTofo6GjY2NhkeB8nT57Eli1bMGXKFHzzzTcG182dOzdbRxpFRkbCysoKanXmB41LkoQVK1agV69euHv3LpYvX260xJQxnndj8fHxwYsXL+Di4oKAgAB07do12W2/+eYb5M2bF/v374eTkxMAkegdNGgQdu3ahZYtW6bpPrt06YL8+fMbJX5jiY2NRefOnREaGor9+/cnStRPmTIFP//8c4r7iIiIgJ2dXVaGmarevXtj3rx5WLlyJb7++utE169YsQJqtTpNI0dTYmVllanbZ5YpHDtERJQ2LOUjIqJU9ezZEy9evEBgYKB+XXR0NAICAtCrV68kb/Pu3Tt88cUX8PDwgLW1NcqUKYPp06dDkiSD7aKiojBy5Ei4urrC0dER7du3x8OHD5Pc56NHj/Dxxx+jYMGCsLa2RoUKFbBw4ULjPdB0srGxQefOnbFixQqD9StXrkTevHnh6+ub5O2uXbuGLl26wMXFBTY2NqhevTo2b95ssM3Lly8xevRoVKpUCQ4ODnByckLr1q1x/vz5RPv77bffUKFCBdjZ2SFv3ryoXr26QUz9+vVLchRYUj14VCoVhg4diuXLl6NChQqwtrbGjh07AGT8+b99+zYAkchLSKPRIF++fAbrHj16hAEDBqBQoUKwtrZGsWLF8OmnnyI6Olq/zZ07d9C1a1e4uLjAzs4OtWvXxtatWw32I5drrlq1CuPGjUPhwoVhZ2eHsLAwAMDx48fRqlUr5MmTB3Z2dmjUqBEOHz6c6uORHT58GPfu3UOPHj3Qo0cP/UiThHQ6HebMmYNKlSrBxsYGrq6uaNWqlUGJZ0rP+9mzZ9G6dWs4OTnBwcEBzZo1w7FjxwzuIyYmBpMmTUKpUqVgY2ODfPnyoX79+gbH7JMnT9C/f38UKVIE1tbWcHd3R4cOHQz6AiXF0dERLi4uqT4fYWFhCAwMRJ8+ffRJKQDo27cvHBwcsGbNmlT3kVZr166Fj48PbG1tkT9/fvTp0wePHj1K9Xbpeb9JaN26dTh//jy+/fbbJEvhnJycMGXKFP3vjRs3RsWKFXH69Gk0bNgQdnZ2+sTs06dPMWDAABQsWBA2NjaoUqUKFi9enGifq1atgo+PDxwdHeHk5IRKlSphzpw5+uvT8ndPqF69evDy8kr0viXvLyAgAE2aNEGhQoVw4cIF9OvXD8WLF4eNjQ3c3Nzw8ccf48WLF6k+X0n1mHr48CE6duwIe3t7FChQACNHjkRUVFSi2wYFBaFr164oWrQorK2t4eHhgZEjR+L9+/f6bfr164fff/8dAAxKPmVJ9ZhKy7Ekl7AfPnwYo0aNgqurK+zt7dGpUyc8e/Ys1cdNRETpxxFTRESUKi8vL9SpUwcrV65E69atAQDbt2/Hmzdv0KNHD/z6668G20uShPbt22Pfvn0YMGAAqlatip07d+LLL7/Eo0ePMGvWLP22AwcOxLJly9CrVy/UrVsXe/fuxQcffJAohtDQUNSuXVt/Au/q6ort27djwIABCAsLw4gRIzL02N6+fZtoxJeLi0uaR9T06tULLVu2xO3bt1GiRAkAYsRBly5dYGlpmWj7y5cvo169eihcuDC+/vpr2NvbY82aNejYsSPWrVuHTp06ARCJl40bN6Jr164oVqwYQkNDMX/+fDRq1AhXrlxBoUKFAIheVsOGDUOXLl0wfPhwREZG4sKFCzh+/HiyScPU7N27F2vWrMHQoUORP39+eHl5Zer5l8uFli9fjnr16qVYXvP48WPUrFkTr1+/xuDBg1G2bFk8evQIAQEBiIiIgJWVFUJDQ1G3bl1ERERg2LBhyJcvHxYvXoz27dsjICBA/xzKvv/+e1hZWWH06NGIioqClZUV9u7di9atW8PHxwcTJkyAWq3GokWL0LRpUwQFBaFmzZqpPk/Lly9HiRIlUKNGDVSsWBF2dnZYuXIlvvzyS4PtBgwYAH9/f7Ru3RoDBw5EbGwsgoKCcOzYMVSvXj3F5/3y5cto0KABnJycMGbMGFhaWmL+/Plo3LgxDhw4oC9vmzhxIqZOnYqBAweiZs2aCAsLw6lTp3DmzBm0aNECgCinvHz5Mj7//HN4eXnh6dOnCAwMxP3799NcvpqSixcvIjY21uAxAWLkTNWqVXH27Nk07+vly5cGv2s0GuTNmxeASBz0798fNWrUwNSpUxEaGoo5c+bg8OHDOHv2LJydnZPdb1rfb5IiJ48/+uijND+OFy9eoHXr1ujRowf69OmDggUL4v3792jcuDFu3bqFoUOHolixYli7di369euH169f60dfBgYGomfPnmjWrJl+JNbVq1dx+PBh/TZp+bsnpFKp0KtXL/z444+4fPkyKlSooL9ux44dePnyJXr37q2P4c6dO+jfvz/c3Nxw+fJl/PXXX7h8+TKOHTuW5ubygCita9asGe7fv49hw4ahUKFCWLp0Kfbu3Zto27Vr1yIiIgKffvop8uXLhxMnTuC3337Dw4cPsXbtWgDAJ598gsePHyMwMBBLly5N9f7TeizJPv/8c+TNmxcTJkzAvXv3MHv2bAwdOhSrV69O82MmIqI0koiIiJKxaNEiCYB08uRJae7cuZKjo6MUEREhSZIkde3aVWrSpIkkSZLk6ekpffDBB/rbbdy4UQIg/fDDDwb769Kli6RSqaRbt25JkiRJ586dkwBIn332mcF2vXr1kgBIEyZM0K8bMGCA5O7uLj1//txg2x49ekh58uTRx3X37l0JgLRo0aIUH9u+ffskAEkud+/eTfW5kR9zbGys5ObmJn3//feSJEnSlStXJADSgQMHDJ4/WbNmzaRKlSpJkZGR+nU6nU6qW7euVKpUKf26yMhISavVGtzn3bt3JWtra2ny5Mn6dR06dJAqVKiQYqx+fn6Sp6dnovUTJkyQEn4UACCp1Wrp8uXLBuvT+vwnRafTSY0aNZIASAULFpR69uwp/f7771JwcHCibfv27Sup1WqD5yz+fiRJkkaMGCEBkIKCgvTXvX37VipWrJjk5eWlf97kv3Hx4sUN4tPpdFKpUqUkX19f/T4lSZIiIiKkYsWKSS1atEj2sciio6OlfPnySd9++61+Xa9evaQqVaoYbLd3714JgDRs2LBkH48kJf+8d+zYUbKyspJu376tX/f48WPJ0dFRatiwoX5dlSpVDI7BhF69eiUBkKZNm5bqY0vJ2rVrJQDSvn37kr3u4MGDia7r2rWr5Obmlur+5ddkwkV+/UZHR0sFChSQKlasKL1//15/uy1btkgApPHjxyfalyw97zdJ8fb2lvLkyZPqY5DJr/l58+YZrJ89e7YEQFq2bJl+XXR0tFSnTh3JwcFBCgsLkyRJkoYPHy45OTlJsbGxyd5Han/35Fy+fFkCII0dO9ZgfY8ePSQbGxvpzZs3kiRJSR7XK1euTPR3lt/r4r93NmrUSGrUqJH+d/lxr1mzRr/u3bt3UsmSJRO9ppK636lTp0oqlcrgfWPIkCGJ3sNkCf+maT2W5MfSvHlzg2N05MiRkkajkV6/fp3k/RERUcaxlI+IiNKkW7dueP/+PbZs2YK3b99iy5YtyY7I2bZtGzQaDYYNG2aw/osvvoAkSfomyNu2bQOARNslHH0jSRLWrVuHdu3aQZIkPH/+XL/4+vrizZs3OHPmTIYe1/jx4xEYGGiwJGxknhKNRoNu3bph5cqVAMQoGg8PDzRo0CDRti9fvsTevXvRrVs3/Uit58+f48WLF/D19cXNmzf15UjW1tb6UVtarRYvXryAg4MDypQpY/BYnZ2d8fDhQ5w8eTJDjz8pjRo1Qvny5fW/Z/b5V6lU2LlzJ3744QfkzZsXK1euxJAhQ+Dp6Ynu3bvre0zpdDps3LgR7dq1SzTqRt4PIF43NWvWNCincnBwwODBg3Hv3j1cuXLF4HZ+fn6wtbXV/37u3DncvHkTvXr1wosXL/SP5d27d2jWrBkOHjwInU6X4nO0fft2vHjxAj179tSv69mzJ86fP4/Lly/r161btw4qlQoTJkxI9vHIEj7vWq0Wu3btQseOHVG8eHH9end3d/Tq1QuHDh3SlyU6Ozvj8uXLuHnzZpLx2trawsrKCvv378erV69SfGwZJZdZJTUZgo2NjUEZVmrWrVtncEwuX74cAHDq1Ck8ffoUn332mUEPoQ8++ABly5ZNVM4ZX1rfb5ITFhYGR0fHND8GQDwX/fv3TxSHm5ubwWvH0tJSP3vhgQMHAIi/6bt371Isy0vt756c8uXLw9vbG6tWrdKve/fuHTZv3oy2bdvqSzHjHzeRkZF4/vw5ateuDQDpfs/dtm0b3N3dDXr62dnZYfDgwYm2jX+/7969w/Pnz1G3bl1IkpSukXey9BxLssGDBxscow0aNIBWq0VwcHC675+IiFLGxBQREaWJq6srmjdvjhUrVmD9+vXQarXJNg0PDg5GoUKFEp3ElStXTn+9/FOtVutL4GRlypQx+P3Zs2d4/fo1/vrrL7i6uhos8knf06dPM/S4KlWqhObNmxss8gnvmzdv8OTJE/2SsLxI1qtXL1y5cgXnz5/HihUr0KNHjyRLXG7dugVJkvDdd98lehxy4kJ+HDqdDrNmzUKpUqVgbW2N/Pnzw9XVFRcuXMCbN2/0+/zqq6/g4OCAmjVrolSpUhgyZEi6+iQlpVixYga/G+P5t7a2xrfffourV6/i8ePHWLlyJWrXrq0vXZPvJywsDBUrVkxxX8HBwYleI0Di11dyj0c+iffz80v0eBYsWICoqCiD5zgpy5YtQ7FixWBtbY1bt27h1q1bKFGiBOzs7PRJFED01ypUqFCaejQl9bxHREQk+1h1Oh0ePHgAAJg8eTJev36N0qVLo1KlSvjyyy9x4cIF/fbW1tb4+eefsX37dhQsWBANGzbEL7/8gidPnqQaV1rJyYSkegZFRkbqr4+OjjY4rp48eQKtVmuwfcOGDQ2OSbk/mfy3Teo5KVu2bIpJg7S+3yTHyckJb9++TdO2ssKFCydqAh4cHIxSpUolKhdO+Pr97LPPULp0abRu3RpFihTBxx9/rO87Jkvt7/7+/ftEz7Wsd+/euHv3Lo4cOQIA2LhxIyIiIvRlfIBIpg8fPhwFCxaEra0tXF1d9a/T1I6RhIKDg1GyZMlE741JPf/3799Hv3794OLiAgcHB7i6uqJRo0YZul8gfceSrGjRoga/y6WkWZXYJSLKzdhjioiI0qxXr14YNGgQnjx5gtatW6fYy8WY5NErffr0gZ+fX5LbVK5c2ej3O3z4cIOGxI0aNcL+/fsTbVerVi2UKFECI0aMwN27d5MdSSY/jtGjRyfbGL1kyZIAgB9//BHfffcdPv74Y3z//ff6vlcjRowwGM1Trlw5XL9+HVu2bMGOHTuwbt06/PHHHxg/fjwmTZoEIPHIHFnCZIAs/miF+HEb6/l3d3dHjx498OGHH6JChQpYs2YN/P3903z79Eru8UybNg1Vq1ZN8jYODg7J7i8sLAz//vsvIiMjUapUqUTXr1ixAlOmTElX/52k4kyPhg0b4vbt29i0aRN27dqFBQsWYNasWZg3b55+psARI0agXbt22LhxI3bu3InvvvsOU6dOxd69e+Ht7Z3h+5a5u7sDAEJCQhJdFxISou+LduTIETRp0sTg+rt37xqlz1VWKlu2LM6ePYsHDx7Aw8MjTbfJzN+0QIECOHfuHHbu3Int27dj+/btWLRoEfr27at/X0rt77569epEI7ak/yag6NmzJ8aMGYMVK1agbt26WLFiBfLmzYs2bdrot+3WrRuOHDmCL7/8ElWrVoWDgwN0Oh1atWqV6qjCjNJqtWjRogVevnyJr776CmXLloW9vT0ePXqEfv36Zdn9JqTRaJJcLyWYwIOIiDKPiSkiIkqzTp064ZNPPsGxY8dSbADr6emJ3bt34+3btwajpq5du6a/Xv6p0+lw+/Ztg2+yr1+/brA/eQYtrVaL5s2bG/MhpWjMmDHo06eP/nf5G/Ok9OzZEz/88APKlSuXbLJDLiGxtLRM9XHIM2P9888/Butfv36N/PnzG6yzt7dH9+7d0b17d0RHR6Nz586YMmUKxo4dCxsbG+TNm1dfLhdfWktSsur5t7S0ROXKlXHz5k08f/4cBQoUgJOTEy5dupTi7Tw9PRO9RoDEr6/kyCNmnJycMvR41q9fj8jISPz555+J/hbXr1/HuHHjcPjwYdSvXx8lSpTAzp078fLlyzSNmorP1dUVdnZ2yT5WtVptkCBxcXFB//790b9/f4SHh6Nhw4aYOHGiPjEFiMf+xRdf4IsvvsDNmzdRtWpVzJgxA8uWLUvns5BYxYoVYWFhgVOnTqFbt2769dHR0Th37px+XZUqVRKVp6W1fFb+216/fh1NmzY1uO769esp/u3T+n6TnHbt2mHlypVYtmwZxo4dm6bbJBfHhQsXoNPpDEZNJfX6tbKyQrt27dCuXTvodDp89tlnmD9/Pr777jt9Ejulv7uvr2+ypYCFChVCkyZNsHbtWnz33XcIDAxEv3799CO8Xr16hT179mDSpEkYP368/nbpLRuM/7gvXboESZIMkrYJn/+LFy/ixo0bWLx4Mfr27atfn9TjSGvyN73HEhERZS+W8hERUZo5ODjgzz//xMSJE9GuXbtkt2vTpg20Wi3mzp1rsH7WrFlQqVT6mf3knwln9Zs9e7bB7xqNBh9++CHWrVuXZNIiq6bwLl++vEE5kY+PT7LbDhw4EBMmTMCMGTOS3aZAgQJo3Lgx5s+fn+SokviPQ6PRJPpmfu3atfoeVLKE07ZbWVmhfPnykCQJMTExAEQy4s2bNwYlPiEhIdiwYUOyscaX2ef/5s2buH//fqL1r1+/xtGjR5E3b164urpCrVajY8eO+Pfff3Hq1KlE28vPR5s2bXDixAkcPXpUf927d+/w119/wcvLy6BPU1J8fHxQokQJTJ8+HeHh4el+PMuWLUPx4sXxv//9D126dDFYRo8eDQcHB30534cffghJkvSj15J6PMnRaDRo2bIlNm3ahHv37unXh4aGYsWKFahfv76+F1DC14GDgwNKliypL6uLiIhAZGSkwTYlSpSAo6NjkqV3GZEnTx40b94cy5YtMyh5W7p0KcLDw9G1a1cAIsGbXPlsaqpXr44CBQpg3rx5BnFv374dV69eTXGGvbS+3ySnS5cuqFSpEqZMmWLw2pO9ffsW3377bar7adOmDZ48eWKQ3I+NjcVvv/0GBwcHfclawr+pWq3Wj0yUH3tqf3d3d/dEz3V8vXv3xtOnT/HJJ58gJibGoIxPHjGU8HWa1ucrqcf9+PFjBAQE6NdFRETgr7/+MtguqfuVJAlz5sxJtE97e3sASDLxnnCfaT2WiIgo+3HEFBERpUtypVzxtWvXDk2aNMG3336Le/fuoUqVKti1axc2bdqEESNG6EesVK1aFT179sQff/yBN2/eoG7dutizZw9u3bqVaJ8//fQT9u3bh1q1amHQoEEoX748Xr58iTNnzmD37t3J9n/KLp6enpg4cWKq2/3++++oX78+KlWqhEGDBqF48eIIDQ3F0aNH8fDhQ5w/fx4A0LZtW0yePBn9+/dH3bp1cfHiRSxfvtygcS8AtGzZEm5ubqhXrx4KFiyIq1evYu7cufjggw/0o9V69OiBr776Cp06dcKwYcMQERGBP//8E6VLl05zA+PMPP/nz59Hr1690Lp1azRo0AAuLi549OgRFi9ejMePH2P27Nn6k9Eff/wRu3btQqNGjTB48GCUK1cOISEhWLt2LQ4dOgRnZ2d8/fXXWLlyJVq3bo1hw4bBxcUFixcvxt27d7Fu3bpEvXsSUqvVWLBgAVq3bo0KFSqgf//+KFy4MB49eoR9+/bByckJ//77b5K3ffz4Mfbt25eogbbM2toavr6+WLt2LX799Vc0adIEH330EX799VfcvHlTXwIVFBSEJk2a6PtrJeeHH35AYGAg6tevj88++wwWFhaYP38+oqKi8Msvv+i3K1++PBo3bgwfHx+4uLjg1KlTCAgI0O//xo0baNasGbp164by5cvDwsICGzZsQGhoKHr06JFiDHIcAPSN3ZcuXYpDhw4BAMaNG6ffbsqUKahbt67+7/fw4UPMmDEDLVu2RKtWrVK9n9RYWlri559/Rv/+/dGoUSP07NkToaGhmDNnDry8vDBy5Mhkb5ue95vk7nv9+vVo3rw5GjZsiG7duqFevXqwtLTE5cuX9aVwU6ZMSXE/gwcPxvz589GvXz+cPn0aXl5eCAgIwOHDhzF79mz9cTtw4EC8fPkSTZs2RZEiRRAcHIzffvsNVatW1fejSu3vnpoPP/wQn332GTZt2gQPDw80bNhQf52Tk5O+F1lMTAwKFy6MXbt24e7du2nad0KDBg3C3Llz0bdvX5w+fRru7u5YunQp7OzsDLYrW7YsSpQogdGjR+PRo0dwcnLCunXrkuztJH9ZMGzYMPj6+kKj0ST7ek7rsURERArI7mkAiYjIfMjTZp88eTLF7Tw9PRNNWf727Vtp5MiRUqFChSRLS0upVKlS0rRp0wym35YkSXr//r00bNgwKV++fJK9vb3Url076cGDB0lO3x4aGioNGTJE8vDwkCwtLSU3NzepWbNm0l9//aXf5u7duxIAadGiRSnGvG/fPgmAtHbt2tSfiDQ+5oSSe/5u374t9e3bV3Jzc5MsLS2lwoULS23btpUCAgL020RGRkpffPGF5O7uLtna2kr16tWTjh49mmgK9vnz50sNGzaU8uXLJ1lbW0slSpSQvvzyS/1077Jdu3ZJFStWlKysrKQyZcpIy5YtkyZMmJBoqnUA0pAhQ5J8PGl5/pO73U8//SQ1atRIcnd3lywsLKS8efNKTZs2NXjMsuDgYKlv376Sq6urZG1tLRUvXlwaMmSIFBUVZfAcdunSRXJ2dpZsbGykmjVrSlu2bDHYT2p/47Nnz0qdO3fWP3eenp5St27dpD179iT7WGbMmCEBSHEbf39/CYC0adMmSZIkKTY2Vpo2bZpUtmxZycrKSnJ1dZVat24tnT59Wn+blJ73M2fOSL6+vpKDg4NkZ2cnNWnSRDpy5IjBNj/88INUs2ZNydnZWbK1tZXKli0rTZkyRYqOjpYkSZKeP38uDRkyRCpbtqxkb28v5cmTR6pVq5a0Zs2aZB9HfACSXRIKCgqS6tatK9nY2Eiurq7SkCFDpLCwsDTdj/yafPbsWYrbrV69WvL29pasra0lFxcXqXfv3tLDhw+T3Fd86Xm/Sc6rV6+k8ePHS5UqVZLs7OwkGxsbqWLFitLYsWOlkJAQ/XaNGjWSKlSokOQ+QkNDpf79+0v58+eXrKyspEqVKiV6zwoICJBatmwpFShQQLKyspKKFi0qffLJJwb3kdrfPS26du0qAZDGjBmT6LqHDx9KnTp1kpydnaU8efJIXbt2lR4/fpzo+ZLf6+7evWvw+OO/V0mSOLbbt28v2dnZSfnz55eGDx8u7dixQwIg7du3T7/dlStXpObNm0sODg5S/vz5pUGDBknnz59P9N4eGxsrff7555Krq6ukUqkM/t5J/U3Tciwl974tv5/Ej5OIiIxDJUns4EdERERERERERNmPPaaIiIiIiIiIiEgRTEwREREREREREZEimJgiIiIiIiIiIiJFMDFFRERERERERESKYGKKiIiIiIiIiIgUwcQUEREREREREREpwkLpALKbTqfD48eP4ejoCJVKpXQ4REREREREREQ5iiRJePv2LQoVKgS1OuUxUbkuMfX48WN4eHgoHQYRERERERERUY724MEDFClSJMVtcl1iytHREYB4cpycnBSOJnNiYmKwa9cutGzZEpaWlkqHQ5Qr8TgkUh6PQyLl8TgkUh6PQzIlYWFh8PDw0OdgUpLrElNy+Z6Tk1OOSEzZ2dnBycmJbzxECuFxSKQ8HodEyuNxSKQ8HodkitLSQonNz4mIiIiIiIiISBFMTBERERERERERkSKYmCIiIiIiIiIiIkXkuh5TRERERERERErSarWIiYkx6j5jYmJgYWGByMhIaLVao+6bKCFLS0toNBqj7IuJKSIiIiIiIqJsIEkSnjx5gtevX2fJvt3c3PDgwYM0NZwmyixnZ2e4ubll+vXGxBQRERERERFRNpCTUgUKFICdnZ1RE0g6nQ7h4eFwcHCAWs2uPZR1JElCREQEnj59CgBwd3fP1P6YmCIiIiIiIiLKYlqtVp+Uypcvn9H3r9PpEB0dDRsbGyamKMvZ2toCAJ4+fYoCBQpkqqyPr1YiIiIiIiKiLCb3lLKzs1M4EiLjkF/Lme2XxsQUERERERERUTZh/yfKKYz1WmZiioiIiIiIiIiIFMHEFBERERERERFlGy8vL8yePTvN2+/fvx8qlSpLZjMk5TExRURERERERGQmtFpg/35g5UrxU6vNuvtSqVQpLhMnTszQfk+ePInBgwenefu6desiJCQEefLkydD9pZWcAEu4jBs3Ltnb+Pv7Q6VSoVy5comuW7t2LVQqFby8vLIwavPHWfmIiIiIiIiIzMD69cDw4cDDh3HrihQB5swBOnY0/v2FhIToL69evRrjx4/H9evX9escHBz0lyVJglarhYVF6mkGV1fXdMVhZWUFNze3dN0mM65fvw4nJyf97/EfZ1Ls7e3x9OlTHD16FHXq1NGv/+eff1C0aNEsi9NYoqOjYWVlpdj9c8QUERERERERkYlbvx7o0sUwKQUAjx6J9evXG/8+3dzc9EuePHmgUqn0v1+7dg2Ojo7Yvn07fHx8YG1tjUOHDuH27dvo0KEDChYsCAcHB9SoUQO7d+822G/CUj6VSoUFCxagU6dOsLOzQ6lSpbB582b99QlL+fz9/eHs7IydO3eiXLlycHBwQKtWrQwSabGxsRg2bBicnZ2RL18+fPXVV/Dz80PHNGTwChQoYPDYU0tMWVhYoFevXli4cKF+3cOHD7F//3706tUr0fabNm1CtWrVYGNjg+LFi2PSpEmIjY3VXz9z5kxUqlQJ9vb28PDwwGeffYbw8HD99cHBwWjXrh3y5s0Le3t7VKhQAdu2bTN4buLbuHGjQaPyiRMnomrVqliwYAGKFSsGGxsbAMDr168xcOBAuLq6wsnJCU2bNsX58+dTfb4yi4kpIiIiIiIiomwmScC7d2lbwsKAYcPEbZLaDwCMGKFCWFja9pfUfjLq66+/xk8//YSrV6+icuXKCA8PR5s2bbBnzx6cPXsWrVq1Qrt27XD//v0U9zNp0iR069YNFy5cQJs2bdC7d2+8fPky2e0jIiIwffp0LF26FAcPHsT9+/cxevRo/fU///wzli9fjkWLFuHw4cMICwvDxo0bjfWwE/n444+xZs0aREREABAJolatWqFgwYIG2wUFBaFv374YPnw4rly5gvnz58Pf3x9TpkzRb6NWq/Hrr7/i8uXLWLx4Mfbu3YsxY8borx8yZAiioqJw8OBBXLx4ET///HOqybOEbt26hXXr1mH9+vU4d+4cAKBr1654+vQptm/fjtOnT6NatWpo1qxZin8HY2ApHxEREREpRqsFgoKAkBDA3R1o0ADQaJSOiogo60VEAOnMJSRLkoBHj1Tw9HRO0/bh4YC9vXHue/LkyWjRooX+dxcXF1SpUkX/+/fff48NGzZg8+bNGDp0aLL76devH3r27AkA+PHHH/Hrr7/ixIkTaNWqVZLbx8TEYN68eShRogQAYOjQoZg8ebL++t9++w1jx45Fp06dAABz587VjypKTZEiRQx+Dw4ORr58+VK8jbe3N4oXL46AgAB89NFH8Pf3x8yZM3Hnzh2D7SZNmoSvv/4afn5+AIDixYvj+++/x5gxYzBhwgQAwIgRI/Tbe3l54YcffsD//vc//PHHHwCA+/fv48MPP0SlSpX0+0iv6OhoLFmyRF9WeejQIZw4cQJPnz6FtbU1AGD69OnYuHEjAgIC0tUTLL2YmCIiIiIiRaTUK6VzZ+XiIiKitKtevbrB7+Hh4Zg4cSK2bt2KkJAQxMbG4v3796mOmKpcubL+sr29PZycnPD06dNkt7ezs9MnpQDA3d1dv/2bN28QGhqKmjVr6q/XaDTw8fGBTqdL9TEFBQXB0dFR/3vevHkBGPaa6tOnD+bNm2dwu48//hiLFi1C0aJF8e7dO7Rp0wZz58412Ob8+fM4fPiwwQgprVaLyMhIREREwM7ODrt378bUqVNx7do1hIWFITY21uD6YcOG4dNPP8WuXbvQvHlzfPjhhwbPX1p4enoa9Po6f/48wsPDEyXg3r9/j9u3b6dr3+nFxBQRERERZTu5V4pK0qIRguCOEITAHYceNkCXLhoEBDA5RUQ5m52dGLmUFgcPAm3apL7dmjXhaNXKDmp1yl177OzSdr9pYZ9g6NXo0aMRGBiI6dOno2TJkrC1tUWXLl0QHR2d4n4sLS0NflepVCkmkZLaXjJSjWKxYsUS9WkCoC95A2DQHF3Wu3dvjBkzBhMnTsRHH32UZCP48PBwTJo0CZ2T+CdnY2ODe/fuoW3btvj0008xZcoUuLi44NChQxgwYACio6NhZ2eHgQMHwtfXF1u3bsWuXbswdepUzJgxA59//jnUanWi5yEmJibRfSX8u4WHh8Pd3R379+9PtG1Sz4UxMTFFRERERNlKqxUjpTpK6zEHw+GBuCFTD1AEI6Q5GDGiMzp0YFkfEeVcKlXay+lathQjSh89Sro/lEoFFCkioWnTWNjbA6nkpbLU4cOH0a9fP30JXXh4OO7du5etMeTJkwcFCxbEyZMn0bBhQwBiVNKZM2dQtWrVDO+3ZMmSKV7v4uKC9u3bY82aNYlGU8mqVauG69evJ7uv06dPQ6fTYcaMGfoE45o1axJt5+Hhgf/973/43//+h7Fjx+Lvv//G559/DldXV7x9+xbv3r3TJ5/iJ9SSU61aNTx58gQWFhbw8vJKdXtjYvNzIqIM0mqBAwdUOHiwMA4cUEGrVToiIiLzEBQE1Hi4HgHogsIwnF6qMB5hLbqg+oP1CApSKEAiIhOj0YgyZ0AkoeKTf585UzKJZH6pUqX0DbXPnz+PXr16pal8ztg+//xzTJ06FZs2bcL169cxfPhwvHr1ymB2uqzg7++P58+fo2zZskleP378eCxZsgSTJk3C5cuXcfXqVaxatQrjxo0DIJJfMTEx+O2333Dnzh0sXbo0UZJrxIgR2LlzJ+7evYszZ85g3759KFeuHACgVq1asLOzwzfffIPbt29jxYoV8Pf3TzXu5s2bo06dOujYsSN27dqFe/fu4ciRI/j2229x6tSpzD0pqWBiiogoA9avB7y8gBYtLDBzZnW0aGEBL6+smaaXiCinefJIizkYDkBK9GFUDTEUYDZG4MkjZvyJiGSdOwMBAUDhwobrixSBSZU/z5w5E3nz5kXdunXRrl07+Pr6olq1atkex1dffYWePXuib9++qFOnDhwcHODr6wsbG5ssvV9bW9sUG6X7+vpiy5Yt2LVrF2rUqIHatWtj1qxZ8PT0BABUqVIFM2fOxM8//4yKFSti+fLlmDp1qsE+tFothgwZgnLlyqFVq1YoXbq0vjG6i4sLli1bhm3btqFSpUpYuXIlJk6cmGrcKpUK27ZtQ8OGDdG/f3+ULl0aPXr0QHBwcKKZBY1NJRmrCNNMhIWFIU+ePHjz5k2SNaHmJCYmBtu2bUObNm0S1dcSUdaR+6IkfPeUv3wxpQ8GRLkB/x+aD50O2LIF2DB8Pxbda5Lq9qem7UP10Y2zPjDKNB6HRKmLjIzE3bt3UaxYsUwlR5KbzVSn0yEsLAxOTk6p9pjKjXQ6HcqVK4du3brh+++/VzqcHCGl13R6ci/sMUVElA5yX5SkUvqSJJJTI0aAfVGIiOKJjASWLQNmzACuXQN6ICRNt1s2LQSPSwPt2iUuXSEiyq00GqBxY6WjMH3BwcHYtWsXGjVqhKioKMydOxd3795Fr169lA6NEmBiiogoGdHRosHkw4fAgwfi57FjhtOaJyRJYtthw4BmzQBPT6BoUSB/fp5UEVHu8+oVMG+e6IsSGirW5ckDNGnjDqxM/fbnnrpjTgfxfjprFlCpUtbGS0REOYdarYa/vz9Gjx4NSZJQsWJF7N69W9+LiUwHE1NElCtFRQGPH8clnOInn+Sf8klURvzxh1hkdnYiQeXpabjI6woVApKYTZaIyCzdvy8SSX//Dbx7J9YVKQKMHAkMGgQ42jUAgopAevgIKiQegipBBalwEdTr0wDHZgN79gBVq4rbTp4MFCiQrQ+HiIjMkIeHBw4fPqx0GJQGPA0iohwnYdIpqeRTWpNO1taAh4c4oSpSRPRHWbEi9ds1bgy8fy9OzkJCgIgIUb5y7VrS22s0Yv9JJa3ky7a2aX4KiIgUcf48MG0asGoV9DOVVqoEjBkDdO8OxLUeEtNLqbp0EWXQ8fYhQYwwVf06G1M6azDwE3H7gABg/nxg5Upg/Hjg888BK6vsfXxERERkfExMEZFZiYpKXF6XcLTT06dp25eNTVzCSU4+xU9CeXgA+fIZluBptcDBgyKGpPpMqVTitrt3x/WYiooScQUHGy7374ufDx4AMTFx65NToEDyiStPT8DZmeWCRJT9JEm8502bBgQGxq1v1gz48kugZctk3ps6dwZmzoRq5EiD1SoA+PNP/SwSxYoBa9eK994RI4CzZ4HRo0WJ4PTpQPv2fO8jIiIyZ0xMEZHJkJNOKZXXpTfplDDRFP9nwqRTWmjEl/zo0kXcNn5ySt7X7NmGjc+trYGSJcWSFK0WePIkccIq/hIeLh7706fAyZNJ78fRMemElby4uQGcoIWIjCU2FlizRiSkzp0T69RqoFs3kZBK08zg8hCqatVEtunnn8Wwq+fPE23asKF4/1uyBPjmG+DWLaBjR5EAmzkTqFzZWI+MiIiIshMTU0SULRImnZJKPqUn6ZRSwimjSae06txZlJQMH27YCL1IEZGU+u9L/jTTaIDChcVSt27i6yVJNBBOLmkVHAw8ewa8fQtcuiSWpFhaiucoqaSVp6eI39o6fbETUe4THg7884/oISWP8rSzAwYMED2kihVLx84OHBA/O3UCevYU/yz69wcWLxbZpwRv5BqNuLpLF2DqVJGQ2rMH8PZm/ykiIiJzxcQUEWVaVFTiJFPC0U7PnqVtX/GTTsmV17m4KF+20bkz0KEDsG9fLLZvP4fWrauiSRMLg5FSxqJSicfs4iJOvpISEZF0uaC8PHokygXv3BFLcvfj5pZ00koeheXkZPzHR0Tm4ckT4LffRJXdq1diXYECotfTp5+KLwTSRZLiElPyvOcffggMGQLcvCmmQa1TJ8mbOjoCP/4oklHsP0VERGTemJgiohRFRqZeXpfWpJOtberldaaQdEorjQZo1EjCu3eP0KhRlSxJSqWVnR1QpoxYkhIbKxrCJ5W0kkdhvX8vGrWHhIjzwaQ4OyeftPL0FCep5vL3I6K0uX4dmDFDlNBFRYl1pUqJyruPPsrExAzXromhsjY2QI0aYp2jo0hOLV0qRk0lk5iSsf8UERGR+WNiiigXi4yMSzYllXBKb9IptfI6c0o65TQWFiKBVLQo0KBB4uslSbR0SS5pFRwMvHwJvH4tlvPnk74fG5vEswnGT2IVLhx/Vi4iMmWHD4v+UZs3x/XTq1NH9I9q3x6ZT8bLo6Xq1DGsI/bzE4mp1atFfbSNTaq7Yv8pIqKcYeLEidi4cSPOyc0LKVdgYopIIVotEBQkRqe4u4tkgTFH3MRPOiXX1ymJ3rJJip90Si75lDcvk07mTKUCXF3FUr160tu8fWuYqErY7+rxY/G6u3FDLElRq0VyKrnEVdGigL191j1OIkqZTicSUdOmAUeOxK1v316UzNWrZ8Q7kxNTjRoZrm/SRPxjefBABNOtW5p2F7//1E8/iVFe7D9FRDlSVp9IxKNK5QP+hAkTMHHixAzve8OGDejYsaN+3ejRo/H5559naH/pMXHiREyaNCnR+sDAQDRv3jzJ2/Tr1w+LFy/GJ598gnnz5hlcN2TIEPzxxx/w8/ODv79/VoScozExRaSA9euTbpw9Z07aGme/f596eV16k04pjXZi0okAUWFToYJYkhIdLV57yTVpv39fbPPggVgOHUp6P/nzpzy7oDFH3mXj5zoikxYZKQYpTZ8el1i2shKlel98AZQrZ+Q7TKq/lEytFnf844+inC+NiSmZoyMwZQowcCDw1Vei1I/9p4gox0jpRCJegsdYQkJC9JdXr16N8ePH4/r16/p1Dg4ORr0/BwcHo+8zORUqVMDu3bsN1rm4uKR4Gw8PD6xatQqzZs2C7X+17JGRkVixYgWKFi2aZbEaS3R0NKxM8J8gE1NE2Wz9evFtrlwWIXv0SKxfvlyMWElptFNGk05JjXpi0omMxcoKKF5cLEnR6YDQ0ORnFgwOBsLCxOv7+XPg9Omk92Nvn3zSqmhRoFChtCWXMpsgJsoJXr4Uzcx/+00cn4DoJffppyKB4+6eRXd886bICFtbA7VqJb7ez08kpnbsiMscp1OxYsCaNSL5PGIEcOYM+08RkZlL7URizRogmdE+GeXm5qa/nCdPHqhUKoN1CxYswIwZM3D37l14eXlh2LBh+OyzzwCIJMioUaOwbt06vHr1CgULFsT//vc/jB07Fl5eXgCATp06AQA8PT1x7969RKV8/fr1w+vXr1G/fn3MmDED0dHR6NGjB2bPng3L//pDhISEYODAgdi7dy/c3NwwZcoUfPPNNxgxYgRGjBiR7GOzsLAweCxpUa1aNdy+fRvr169H7969AQDr169H0aJFUSzBtLQ6nQ4///wz/vrrLzx58gSlS5fGd999hy5dugAAtFotBg8ejL179+LJkycoWrQoPvvsMwwfPly/j/3792PMmDG4fPkyLC0tUaFCBaxYsQKenp7652bjxo367UeMGIFz585h//79AIDGjRujYsWKsLCwwLJly1CpUiXs27cPly5dwpdffomgoCDY29ujZcuWmDVrFvLnz5+u58NYmJgiykZarTgRTvi/BIhb16tX2vZlZ5d6eZ2zMz90k+lQq8W5pbt70uehAPDmTcoN2p88Ad69A65eFUtSLCzEMZBck/aiRYFt21L+XBcQwOQU5WzBwcCsWcCCBeKYAsT/jZEjxUgjR8csDkAeLVWrVtI9pEqXFr2njh4V39iMHp3hu2rQQPSfWryY/aeIyMRIkphaOS20WmDYsORPJFQqqEaMEHXYGo344JUSO7tMnygsX74c48ePx9y5c+Ht7Y2zZ89i0KBBsLe3h5+fH3799Vds3rwZa9asQdGiRfHgwQM8ePAAAHDy5EkUKFAAixYtQqtWraBJ4VvFffv2wd3dHfv27cOtW7fQvXt3VK1aFYMGDQIA9O3bF8+fP8f+/fthaWmJUaNG4enTp5l6bCn5+OOPsWjRIn1iauHChejfv78+GSSbOnUqli1bhnnz5qFUqVI4ePAg+vTpA1dXVzRq1Ag6nQ5FihTB2rVrkS9fPhw5cgSDBw+Gu7s7unXrhtjYWHTs2BGDBg3CypUrER0djRMnTqRaXpnQ4sWL8emnn+Lw4cMAgNevX6Np06YYOHAgZs2ahffv3+Orr75Ct27dsHfvXqM8R+nFxBRRNoiNBc6dAxYuNBydkRxra8DLK+XyOiadKCfKk0ecJCZ3ohgZKUYPJteg/cEDcbzduyeW5KjVKX6uw4gRQIcOLOujnOfsWdE/as0acY4DAFWqiIbm3bpl4+QEyfWXis/PTySmFi8W9YSZ+KenVrP/FBGZoIgIwFhla5IE1aNHcPb0TNv24eGZbuw5YcIEzJgxA53/+zavWLFiuHLlCubPnw8/Pz/cv38fpUqVQv369aFSqeAZLzZXV1cAgLOzc6qjlvLmzYu5c+dCo9GgbNmy+OCDD7Bnzx4MGjQI165dw+7du3Hy5ElU/69R6oIFC1CqVKlU47948aJB2WD58uVx4sSJVG/Xp08fjB07FsHBwQCAw4cPY9WqVQaJqaioKPz444/YvXs36vw3w2zx4sVx6NAhzJ8/H40aNYKlpaVBn6tixYrh6NGjWLNmDbp164awsDC8efMGbdu2RYkSJQAA5TJQW1+qVCn88ssv+t9/+OEHeHt748cff9SvW7hwITw8PHDjxg2ULl063feRWUxMEWWBqCjx7ezBg2I5fFi896fVwoVpHzlFlJvY2Ihp6pP7rKHViqqf5Bq0BweL0SE6XfL3IUkiwXXwoOjBTGTuJAkIDBQJqfitNJo3Fw3NmzfP5i86UuovFV/37mKY8aVLIqNWrVqm75r9p4iIjOPdu3e4ffs2BgwYoB+5BACxsbHIkycPAFGG16JFC5QpUwatWrVC27Zt0bJly3TfV4UKFQxGVLm7u+PixYsAgOvXr8PCwgLV4v2PKFmyJPLmzZvqfsuUKYPNmzfrf7f+b4bYoKAgtG7dWr9+/vz5+tFRgEiqffDBB/D394ckSfjggw8SlcDdunULERERaNGihcH66OhoeHt763///fffsXDhQty/fx/v379HdHQ0qlatCkD0u+rXrx98fX3RokULNG/eHN26dYN7OsvbfXx8DH4/f/489u3bl2Qvr9u3bzMxRWSu3r0TX+rKiahjx0RyKj5nZ9E89ujR1PdXqFCWhEmU42k0caMMk5o9TJKAv/8GPvkk9X116yYSxO3bi6nos20kCZGRxMQAq1eLfkrnz4t1Go3I94weLUYLKeLOHTF82NISqF07+e2cncXQxTVrxKgpIySmZOw/RUQmwc4u7d9eHzwItGmT6mbha9bArlUrqNNSypcJ4f/F/ffff6NWgh4NchKpWrVquHv3LrZv347du3ejW7duaN68OQICAtJ1X5YJPoSpVCroUvqWMY2srKxQsmTJROurV6+u73EFAAULFky0zccff4yhQ4cCEMmlhOTnZ+vWrShcuLDBdXICbNWqVRg9ejRmzJiBOnXqwNHREdOmTcPx48f12y5atAjDhg3Djh07sHr1aowbNw6BgYGoXbs21Go1pAQlADExMYlisU8wMi48PBzt2rXDzz//nGjb9Ca9jIWJKaIMePVKjIKSE1GnT4vyofgKFhQns/JSsaI4KfbyEn1skiojUqnECXWDBtnyMIhyHZVKtK5Ji+fPgV9/FUuePEDr1uJktXVrcb5MZKrevhW9o2bNEqP/AFGtMXCgSML8129WOfJoqZo1Uz8x8vMTGaQVK8SQLyMPZ2L/KSJSlEqV9nK6li3FiUIKJxJSkSKIbdpU7DO1xFQmFSxYEIUKFcKdO3cMRhMl5OTkhO7du6N79+7o0qULWrVqhZcvX8LFxQWWlpbQynXlGVSmTBnExsbi7Nmz+pFBt27dwqtXrzK8T1tb2yQTVvG1atUK0dHRUKlU8PX1TXR9+fLlYW1tjfv376NRMmXrhw8fRt26dfXN4gExYikhb29veHt7Y+zYsahTpw5WrFiB2rVrw9XVFZcuXTLY9ty5c4kSeQlVq1YN69atg5eXFywsTCMlZBpREJm40FDxraqciLpwIfH/g6JFRasMORFVqlTS37bOmSN6XKhUhvuQt509m31tiLJSgwapfq5D4cIiIbV1K/Dvv8DTp8CqVWKxsBDHePv2QLt2yc9CSJTdnjwRr9s//wRevxbrChQQvXI//RRIZQbs7JOW/lKyli0BNzfx4LZvFyOojIz9p4jILGg0qZ5ISDNnZuuJxKRJkzBs2DDkyZMHrVq1QlRUFE6dOoVXr15h1KhRmDlzJtzd3eHt7Q21Wo21a9fCzc0Nzv99w+fl5YU9e/agXr16sLa2TlP5XUJly5ZF8+bNMXjwYPz555+wtLTEF198AVtb23Q3CU8PjUaDq//NxJNU43ZHR0eMHj0aI0eOhE6nQ/369fHmzRscPnwYTk5O8PPzQ6lSpbBkyRLs3LkTxYoVw9KlS3Hy5En97H53797FX3/9hfbt26NQoUK4fv06bt68ib59+wIAmjZtimnTpmHJkiWoU6cOli1bhkuXLhmUCiZlyJAh+Pvvv9GzZ0+MGTMGLi4uuHXrFlatWoUFCxak2Ig+q2RtGpXITN2/DyxbBgweDJQtKz4Td+0qptM+f178HyhdWnz7vGSJaLIcHCwuDxworkvufbBzZzHjV4IRnShShDOBEWUH+XMdkPg4lX+fMwfo1EmMOgkJESW4Y8cCFSqI0ZF794qRJyVKAJUqAd9+Cxw/nnLvKqKscu2a+N/j6QlMnSqSUqVLA3/9Jf43ffutCSWlgLT1l5JZWAB9+ojL/v5ZFRGAuP5TV6+K//k6neg/VaqUSFZFR2fp3RMRpc7ETiQGDhyIBQsWYNGiRahUqRIaNWoEf39/fWLF0dERv/zyC6pXr44aNWrg3r172LZtm77McMaMGQgMDISHh0eqyZSULFmyBAULFkTDhg3RqVMnDBo0CI6OjrBJatZXI3JycoKTk1Oy13///ff47rvvMHXqVJQrVw6tWrXC1q1b9c/PJ598gs6dO6N79+6oVasWXrx4YTB6ys7ODteuXcOHH36I0qVLY/DgwRgyZAg++a8nha+vL7777juMGTMGNWrUwNu3b/VJq5QUKlQIhw8fhlarRcuWLVGpUiWMGDECzs7OqZeAZhGVlLAoMYcLCwtDnjx58ObNmxRfROYgJiYG27ZtQ5s2bVIdrkfJkyQxdF8eDXXggPggH59KJU4+5dFQDRqIZFVmaLViFFZICODuLvbJkVLmh8eh+Vq/XvRVjj9TpoeHGLWY0ue627fFKKpNm8QxHH8EesGCYhRV+/aiFCiT7RsojXLjcShJoqR82jQgXt9W1K0rZthr3z7Lqzgy5t490eDJwkJk0NJSwnLpkvgnbGkJPH4MJGgwm1Xi958CgJIl2X8qJbnxOCRKr8jISNy9exfFihXLXNIkmRMJnU6HsLAwODk5KZZgMAUPHz6Eh4cHdu/ejWbNmikdTo6W0ms6PbkXlvJRrqPTAZcvxyWiDh4UFQLxaTSAj09cIqpePeN/26zRpO3LYiLKGp07i6qg9CaIS5QQJ6sjRgAvX4rqos2bxc/QUDHKasECwNYWaNFCnMS2bSuSVkSZpdWK19svv4iJNgCRJOnQQSSk6tZVNr5UyaOlqldPe1+VihVF4/MzZ8T0eZ9/nnXxxcP+U0RksngiYWDv3r0IDw9HpUqVEBISgjFjxsDLywsNGzZUOjRKIyamKMeLjRWzTMtJqKAg0bw8PmtroFatuERUnTpAErNnElEOk9nPdS4uQO/eYomOFufcmzeL5f79uMsqlXiPad9eLOXLc8QFpc/796JcfMYM4OZNsc7aGujbF/jiC6BMGWXjS7P09JeKz89PJKYWL862xBTA/lNEROYgJiYG33zzDe7cuQNHR0fUrVsXy5cv5+hNM8LEFOU4kZHiG045EXXkSOJZWO3txSgoORFVowaQxSXIRJTDWVmJEVItWogG1BcuxCWmTp0So1uOHRMjL4oXj0tS1a8vKpSIkvLiBfDHH6LH4bNnYp2zM/DZZyI/k9my8myXnv5S8fXsKTJwp0+LYc8VKhg9tJTI/acGDgS++gpYu1b0n1q5EvjuO/G3+G/2byIiyma+vr5JzoxH5oOJKTJ74eGiMbGciDp+HIiKMtwmb14xJF9ORFWtyhNBIso6KhVQpYpYvvtOzAC4ZYtIUu3ZA9y5I3pZzZ4t3p/atBFJKl9fIE8epaMnU3DvnigX++cfICJCrCtaFBg1ChgwwExH9T54IF78Go34dig9XF2BDz4Qzd0WLxa1jAooVgxYs8aw/9SXXwLz5onRVOw/RURElH5MTJHZefVKNHyVG5WfPm3YfBgQvVzkJFTDhqI9RS7u/0dECitcGPjkE7GEhwOBgSJJtWUL8Pw5sHy5WCwtxUCS9u1FE3VPT6Ujp+x25oxoaL52bdz/tqpVRfKja1cz/1JFHi1VrZoYgpRefn4iMbV0KfDjj6KBukIS9p+6fVv0n2raFJg1i/2niIiI0oOJKTJ5oaHim0l5RNSFC2I2ovg8PQ0TUaVK8RtLIjJNDg5Ap05i0WpFeZ9c8nftmkhaBQaK0qAqVeJK/qpVY4I9p5IkYNcuMQho79649S1bioRUs2Y55H9aRsv4ZB98AOTLJ2YsCQwEWrc2WmgZkVT/qb17Rf+pgQOB779n/ykiSppOp1M6BCKjMNZrmYkpMjn37xvOmHf9euJtypSJS0I1aMBRBURknuSKpnr1gJ9/Bm7cAP79VySpDh0Czp8Xy/ffA4UKiVFU7duLURnsi2f+YmKAVauA6dPFly6AeE306AGMHi1GSuUoGW18LrOyAnr1Eg23Fi9WPDElS6r/1F9/ib8t+08RUXxWVlZQq9V4/PgxXF1dYWVlBZURv3nQ6XSIjo5GZGQk1Pw2i7KQJEmIjo7Gs2fPoFarYWVllan9MTFFipIkMbtQ/ERUcLDhNiqVGBIfPxHFadeJKCcqXVr0d/7iC9H0ets2kaTasQN4/Fg0W54/H7CzE/2o2rcXg0hcXZWOnNLj7Vvg779FydfDh2KdvT0weLDoW1S0qKLhZY3Hj8U/fLVadPzPKD8/kZjauBF4/Vp0gjcR7D9FRKlRq9UoVqwYQkJC8PjxY6PvX5IkvH//Hra2tkZNeBElx87ODkWLFs10IpSJKcpWOh1w6ZJhIio01HAbjQbw8RFfqDZsKEYS5M2rTLxERErJlw/46COxREUB+/fHlfw9fAhs2CAWlQqoWzeu5K9MGZ78mqrHj8WMjfPmAW/eiHUFCwLDhwP/+18O/18nj5aqWjVzHf6rVRMz8l2+LLJAgwcbJTxjkvtPLVkCjB3L/lNEZMjKygpFixZFbGwstAkb5WZSTEwMDh48iIYNG8LSrJsSkjnQaDSwsLAwShKUiSnKUjExwNmzcUmoQ4dE8/L4rK2BWrXiRkTVqWOmsw0REWURa2sxQsrXF5g7Fzh3TvSA3rxZvMcePiyWr74SPfbkJFXduor2h6b/XL0qyvWWLhX/FwGRQBw9GujTJ5eUZWa2v5RMpRKjpsaMEeV8JpiYAsTAsH79gA8/ZP8pIkpMpVLB0tLS6MkjjUaD2NhY2NjYMDFFZoUfV8moIiPFt4RyIurwYeDdO8Nt7O3FKCg5EVWjRi75UE5EZAQqlTi59fYGJk4EHjyI60u1d6+olpoxQywuLqLUr317kdTKyERolDGSJL6M+eUXMfuirH59Ud7Vtm0ua2a/f7/4mdH+UvH16QN8/TVw5Ih4wZcqlfl9ZhG5/9SgQSJxvGYN+0+RadJqRRlqSAjg7i5G/mk0SkdFRLkFE1OUKeHhwNGjcYmo48dFyUl8efOKf25yIsrbm9/gExEZi4cH8NlnYgkLE7O7bd4MbN0KvHwpRuksXSr6RjdpIpJU7dqJ25HxabWi/dG0aeJ/IiCSiR07ioRUnTpKRqeQJ0/ETCYqlfhAkFnu7mLKwh07xKipH37I/D6zmJcXsHo1MHQo+0+R6Vm/XpQUyz3vAKBIEWDOHKBzZ+XiIqLcg+kBSpdXr8Q3wHIi6vRp8SE8voIF4/pDNWwoWkHkqm+FiYgU4uQkpq7v0gWIjRUDSjZvFmV/t24BO3eKZcgQ8SWBXPLn7c2T4sx6/17kSGbMEM81IEbC9OsHjBolGtvnWgcPip+VKxuvkVa/fiIxtXQpMHmy2XzQYP8pMjXr14v/GZJkuP7RI7E+IIDJKSLKekxMUYqePBHDeuVE1MWLif9xeXnFJaEaNgRKluQJDhGR0iws4t6Xp00TA1bk5ulHjojeVGfPApMmiW/G27UTSaomTVhalB4vXgC//y56fz17JtblzSuSf0OHchZZAMbrLxVfhw6iifr9+6JMsGlT4+07i7H/FJkKrVaMlEr42R4Q61QqMcKvQweW9RFR1mJiigwEBxvOmHfjRuJtypSJGxHVoEEOndaaiCgHUamAsmXFMmaMSKBs3SqSVDt3ivKNP/8Ui4MD0KqVSFK1aSNmB6TE7t4FZs4E/vlHjJYCAE9PMTrq4485iYcBY/aXktnYAN27i4ZNixebVWJKxv5TpLSgIMPyvYQkSfQxDAoybl6ZiCghJqZyMUkSPUPlJNSBA+KLx/hUKjGsXP7WvUEDfvtLRGTuXF3FiI1+/cSkFXv3xo2mCgkRpRsBAWJkR/36cSV/JtxjOtucOiVGoAUEADqdWOftLRJ+Xbqwh2Iiz54BV66Iy8boLxWfn5/I5KxbJ4atmWk2kP2nKDtFRor3sUOHxOsuLSZPBu7cET3yypQxm8pZIjIj/PhkprRa4MABFQ4eLAx7exWaNEl9iK1OB1y6ZDgiKjTUcBuNBqhePS4RVa+e8dpBEBGR6bGxESOj2rQB/vhDnBTLSarz5+P+X4weLUZcyUmq2rVzT2mHJIl2RtOmAfv2xa339RUJhKZNmThIltxfqmJFIH9+4+67Th2RLb15UySn/PyMu/9sxv5TlBVevhTl24cOieXkSSA6On372Lcv7r3P2RmoVUscfnXqADVrinVERJnBxJQZips5wwJAdcycmfTMGTExon+IfFIRFAS8fm24L2trcXIhJ6Jq1zbbLxyJiCiT1Grx5UT16uIb8uBg4N9/RZJq3z7g2jWx/PKLyDG0bSuSVC1a5Mz/HdHRoqxq2jTxxQ4gRkT17CkSdUwUpEFW9JeSqVRA376i7m3xYrNPTAHsP0WZI0mi+kFOQh06FPfeFV/BgiIRWreueJ09e5Z0nymVSpRz9+8PnDghklqvX8dNpCFvU65cXKKqdm3xO0dVEVF6qCQpqbehnCssLAx58uTBmzdv4OTkpHQ46ZbczBnyN7WTJ4ufBw+Kb0fevTPczsFBjIKSE1E1arB/AVFmxMTEYNu2bWjTpg0sLS2VDocoy7x5I05ENm8W/anif9FhbQ00ayaSVG3bAoULZ29sxj4Ow8JEhdjs2WJmKkD8/xw8WJRaeXhk+i5yj8qVxcwpa9eKDzDGFhwsauEA0fhLvpxD3LsX138KEDNvmmr/Kf4/zH5aLXD5svjyWU5EJdUzqkwZUZYtLyVKxJ07yOcWgOH5hXx9/Fn5YmOBCxeAY8eAo0fFcvt24vvLkyduVFXt2uIyKzCyB49DMiXpyb0wMWVGtFrxeSulJoUJ5c0rvhGRm5VXrcr+F0TGxA8AlBvFxACHD4sk1aZNovdIfNWrx5X8Va6c9WVuxjoOHz8Wo4/nzRPJKQBwcxOjlP/3P5arpNuLF3Hle6GhWTfUp1kzMaxo8mSRtcmBgoLi+k8BIrFgav2n+P8w671/L0YtyUmoI0fElwbxWVgA1aqJBJQ8Kiq1Qy+uGiNunYeHSM7Hr8ZIytOnwPHjcYmqEyeAiIjE28mjqmrXFj/Ll+eoqqzA45BMCRNTKTDnxNT+/WIa79Q0biy++WjYEKhQgW/6RFmJHwAot5Mk4OpVkaDavFmcoMT/ZFG0aFySqlEjwMrK+DFk9ji8fBmYPh1Yvlwk3QDRT+vLL4HevU1vZIrZ2LgR6NRJnJHKDdCzwpIlooyvZEkxnbCpZGqMTKeL6z/15IlYZ0r9p/j/0PhevIjrDxUUJJqWy+9RMgcHkXySR0PVrAnY26f/vrRacR8hIYC7u0hqZaSPYGysKB+UE1VHjwK3biXezslJjKSSE1W1a3NUlTHwOCRTkp7cC8fOmJGQkLRtN3iw6H9BRESU1VQq8c13+fJxJ8xbt4okVWCg6Hcyd65YnJyAVq1Ekqp1a8DFRbm4JUmUvU+bJuKVNWggElIffMAvdjItK/tLxde5M/DZZ+Ls98gR0bMgB2L/qZxNkkTpZvz+UEnlc93cxPuUnIiqXNk41RAajXEOVQsLUaFRtSrw6adi3fPnhuV/J06IUamBgWKRlS0bl6iSR1Xllkk2iHI7JqbMiLu7cbcjIiIyNjc3YMAAsUREAHv2iCTVv/+Kaq41a8Si0YiTK3k0VYkS2ROfVgts2CASUidOiHUqlRjY8+WX4qSIjGT/fvGzUaOsvR8HBzFUfPFiseTQxJTM0RGYMgUYNCiu/9Rff4lG/ePGAcOGcZSfOdBqRfu1+IkouaddfGXLxpXl1a8PFCtmfoMC5cky2rYVv2u1hqOqjh0Tgx3lCTb8/cV2jo5iBFj8xupKfqFBRFnHJEr5fv/9d0ybNg1PnjxBlSpV8Ntvv6FmzZpJbuvv74/+/fsbrLO2tkZkZGSa7sucS/nkHlOPHiU/c0aRIqL3J79dIMoeHDJNlDY6neiNsnmzWBLOFFW+fFySqlat9I1WSstxGBEhTnZmzoxr1mtjI0agjBoFlCqVoYdFyXn1SkznJUliyLebW9be3759oq7NyUkM27O1zdr7MyFJ9Z+aPh3o0CF7Exj8f5iy9+9FMlwuyzt6NK6XnczCQvTok0dD1a0LuLoqE292e/EiblTVsWOiLDw8PPF2pUsbJqoqVuR5T3w8DsmUmFUp3+rVqzFq1CjMmzcPtWrVwuzZs+Hr64vr16+jQDLjkZ2cnHD9+nX97ypz+9oggzQa0ZS1SxfxQSOpmTNmz+abMxERmR61WiScatUSoz3u3BGjqDZvFhVfV66I5aefRDlS27YiSdWiBWBnl/x+tVrgwAEVDh4sDHt7FZo0Mfw/+Pw58PvvopTw+XOxzsUFGDoUGDKEpU9Z5tAh8UGldOmsT0oBYlSWp6eYpW/TJqBHj6y/TxPRoIFI+sr9p27fFiMAmzQRnwtNof9UbvT8uZgkQh4Ndfp04v5Qjo6J+0Ol9H6Xk+XLJ0qoP/hA/C7POBi/BPD6dTGy6sYNMTgSEAMm5VFVtWuLRZ5zgYjMh+IjpmrVqoUaNWpg7ty5AACdTgcPDw98/vnn+PrrrxNt7+/vjxEjRuB1/Hmq08GcR0zJMjNzBhEZF7+ZIsq8V6+AHTtEkmrbNsNRBDY2IjnVvr1IVsXPcST1/7BIEfElTpUqYnTUokVipAIgSmBGjQL6989Yc2BKh9GjRROkwYOB+fOz5z6/+w744QfA11e8oHKht2/j+k9FRYmEcHb1n8rN/w8lSVQsxC/Lu3o18XaFChn2h6pUiV8op8fLl4YzAB4/Ll7zCZUqZTgDYMWKuWdW8tx8HJLpMZtZ+aKjo2FnZ4eAgAB07NhRv97Pzw+vX7/Gpk2bEt3G398fAwcOROHChaHT6VCtWjX8+OOPqFChQpL3ERUVhaioKP3vYWFh8PDwwPPnz802MQWIbxH279ciMPASWrSoiMaNNfzHRqSAmJgYBAYGokWLFvwAQGQE0dHAoUMqbNmiwr//qhEcbDgqumZNHdq2lWBrK2HMGM1/o4fjbyM+1oiRxWJ9tWo6jBqlQ+fOUq45OVGapnZtqM+cQezixZCya0aWW7dgWb48JLUasXfuiCxALnXvHvDNNxoEBIiaWCcnCWPH6jB0qC7L+k/lpv+HWi1w4QJw5Igahw6pcOSICiEhiSs4ypWTUK+eDnXrSqhXT4KXl/n1hzJlWq1IAB4/rsKxY2ocO6bC9euJn2B7ewk1akioWVNC7doSatWScmyJZG46Dsn0hYWFIX/+/KafmHr8+DEKFy6MI0eOoE6dOvr1Y8aMwYEDB3D8+PFEtzl69Chu3ryJypUr482bN5g+fToOHjyIy5cvo0iRIom2nzhxIiZNmpRo/YoVK2CXW8fKEhERmQFJAoKDHXHypDtOnHDDzZsJ5xKXYJiUMuTtHYrOnW+hYsXnPBnMRhbv3qHNRx9BpdNh5z//IDJfvmy77/pjxyLf1au43LcvbnEYOS5fdsHChZVw+7YzAMDNLRz9+l1GrVpPeEykQ1SUBjdu5MWVKy64ejUfrl/Pi/fvDU/6LSx0KFHiNcqVe4Hy5V+ibNkXcHKKSWaPlFXCwy1x40ZeXL+eF9euueDmzbyIiEicoHF3D0fp0q9QpsxLlC37Cp6eYdBoFG+9TJSjREREoFevXjkzMZVQTEwMypUrh549e+L7779PdH1OHTEFMCNOZAp4HBJln5AQYNs2Ffz91Th+PPXu6IGBsWjUiCca2U21fTssOnSAVLIkYpOa7z4r7/uff2Dx6aeQypVD7LlzHJ4CMfHAsmUqjBunwZMn4vlo3FiH6dO1Ru0/lZP+Hz57Bhw5IkZCHT6swpkzKsTGGr6WnJwk1K0r6UdDVa8u5aae+2ZDp0s8quratcTvC3Z24m9Yq1bcqCpz7EGYk45DMn/pGTGl6ID2/PnzQ6PRIDQ01GB9aGgo3NLYKNPS0hLe3t64detWktdbW1vDOokxy5aWljnmYM1Jj4XIXPE4JMp6RYsC//sfkCcP0KtX6ts/e2YBHpYKOHwYAKBq1Cj73xd79gRGjoTq6lVYXrggpjgjDBgAdO8u+k9Nnw7s369GzZrqLOk/ZW7/DyVJTMYgz5Z36JBosp1Q4cKG/aEqVlRBo2Hi0xxUrSqWTz4Rv79+nbhX1Zs3Khw8qMLBg3G3K17ccAbAypVhNv9TzO04pJwpPa9BRRNTVlZW8PHxwZ49e/Q9pnQ6Hfbs2YOhQ4emaR9arRYXL15EmzZtsjBSIiIiMhXu7sbdjoxs/37xs1Gj7L/vPHmAjh2BVavEtF1MTOk5OIje8AMHAl99BaxZA/z1l3iqxo0Dhg1DlvWfMiWxscD584aNyp88SbxdhQoiASUno4oW5QC8nMLZWcyR4OsrftfpgGvXRJJKngXwyhWRsLxzB1i+XGxnawvUqGHYWL1gQcUeBlGOongL0FGjRsHPzw/Vq1dHzZo1MXv2bLx79w79+/cHAPTt2xeFCxfG1KlTAQCTJ09G7dq1UbJkSbx+/RrTpk1DcHAwBg4cqOTDICIiomzSoIGYfe/RIyCphgQqlbi+QYPsjy3Xe/sWOH1aXFYiMQUAfn4i27JihRgelBuyLeng5QWsXg18/jkwYoT4c40ZIyZPnD4d6NAhZyVg3r0TI2LkEVFHj4p18VlZiYSDPBqqbl3AxUWZeCn7qdVA+fJiGTBArHvzRrxu5ETVsWNipNXBgzAYVVWsmGGiqkoV8xlVRWRKFE9Mde/eHc+ePcP48ePx5MkTVK1aFTt27EDB/9LP9+/fh1od10fi1atXGDRoEJ48eYK8efPCx8cHR44cQfny5ZV6CERERJSNNBpgzhygSxd59r246+QT6tmzOQ27Io4cEVNlFSsmhpgooUULMVwuJATYuhVgE/Qk1a8PnDgBLFkCjB0L3L4NdOoENGkijh9j9p/KTk+fGo6GOnNGvCTjy5MHqFcvLhFVowZgY6NMvGSa8uQBWrYUCyBGVV2/HpeoOnoUuHwZuHtXLCtWiO1sbcVATTlRVacOkMYONUS5muKJKQAYOnRosqV7++Xh4P+ZNWsWZs2alQ1RERERkanq3BkICACGDwcePoxbX6SIOKlmLkIhSpbxyTQa4KOPgF9+EeV8fDEkS60G+vUTSV65/9S+fYC3N7Kk/5SxSRJw65ZhIurGjcTbeXgY9oeqUEE8dqK0UquBcuXE8l9hD968AU6ejEtUHTsGvHolRuYFBcXd1svLMFFVpYoYpUdEcUwiMUVERESUXp07i7KjfftisX37ObRuXRVNmlhwpJSSDhwQP5VMTAGinO+XX4Bt28QUa66uysZj4syl/1RsLHDunGEiKsEcSlCpgIoV45JQcn8oImPLkwdo3lwsgBhVdfNmXKLq6FHg0iXg3j2xrFoltrOxAXx8DBurFyqk1KMgMg1MTBEREZHZ0miARo0kvHv3CI0aVWFSSknv3onhA4Dyiany5UU9zalTosZm+HBl4zETptZ/KjxcjEKRk1DHjiXdH6pmzbhG5XXqAHnzZl+MRDK1GihTRiz9+ol1b9+Kktn4o6pevhSTl/43gSkAkTyNn6jy9k7/qCqtFjhwQIWDBwvD3l6FJk1Y0k7mg4kpIiIiIsq8o0fFkJaiRUWGQ2l+fiIxtXgxE1PppFT/qSdPxMm6nIg6ezZxfyhnZ8PRUD4+7A9FpsvREWjWTCyAKD+VR1XJ/aouXgTu3xfL6tViO2tr8dqOXwJYuHDy97N+vVzabgGgOmbOFKXtc+awmpnMAxNTRERERJR58ftLmcK0bj17AqNGiezGxYtApUpKR2RW0tN/KiMjNeQTdHm2vEOHRL+ohDw9DRNR5cuzPxSZL5UKKF1aLH5+Yt3bt2KwafzG6i9eiLkkjhyJu62Hh2GiyttbJLDWrxfHacJZah89EusDApicItPHxBQRERERZZ6p9JeS5csHtG0LbNggRk1Nn650RGYptf5TRYsCo0enPlIjJkbkCOP3h3r2zPC+VCqRP5QblderJ07GiXIyR0egaVOxAHFN/eMnqi5cAB48EMvatWI7KyuRnLp8OXFSSt6PSiXKcjt0YFkfmTYmpoiIiIgoc96/F7VfgOkkpgAxJGHDBmDZMjHsx4IffTMquf5TSZFHanz3nfhd7g8VEWG4nbU1UKtW3GioOnVEqR5RbqZSAaVKieWjj8S68HBRmRy/sfrz58Dx4ynvS5JEMisoCGjcOMtDJ8ow/ncmIiIiosw5dgyIjhZNUEqUUDqaOG3aiBn5QkOBnTuBDz5QOiKzJ/ef8vcHBg0SM5ElJI/emDzZcL2LixgFFb8/lCnM9kdk6hwcRGJJTi5JEnDnDjBzJvDHH6nffsQI8fbn7S2W4sVNo+KaSMbEFBERERFljqn1l5JZWgK9eom6ssWLmZgyErVanNgmlZRKqEULMXqqfn2gbFn2hyIyBpVKfAfQtWvaElPnz4tF5uQEVK0qklTVqomfZcuKt0wiJTAxRURERESZY2r9peLz8xOJqU2bgFevgLx5lY4oRwgJSdt2/fuLPvREZHwNGoiebo8eJd1nSqUSg0bHjxeJqTNnxFwQYWHAwYNikVlbix5v8qgqb28xA6edXfY9Hsq9mJgiIiIiooyLjBSlfIBpJqaqVhVnWxcviiZJ//uf0hHlCO7uxt2OiNJPoxF59y5dRBIqfnJKHrz655+JJyK4elVMRiAv586JZNWpU2KRqdViJFX8ZJW3N/P7ZHxMTBERERFRxp04AURFAW5uYg50U6NSiVFTo0eLcj4mpowiLSM1ihQR2xFR1uncGQgIAIYPBx4+jFtfpAgwe7ZhUgoQ5XqVK4vFz0+s0+mAu3fFiKr4CavQUODKFbEsXx63D09PwzJAb2+gUCHTquQm88LEFBERERFlnKn2l4qvd2/gq6/EyK7r14EyZZSOyOylZaTG7Nmcop4oO3TuDHToAOzbF4vt28+hdeuqaNLEIs3Hn1otelbJfatkISGGiaozZ0QCKzhYLBs3xm3r6pp4ZFXJkuwrR2nDxBQRERERZZwp95eSubkBvr7Atm1i1NSPPyodUY6Q3pEaRJR1NBqgUSMJ7949QqNGVYySFHZ3F0ubNnHrXr8WpX/xE1ZXrwLPngG7dolF5uAQ12RdXsqXB6ysMh8b5SxMTBERERFRxkRHA0ePisumnJgCgH79RGJq6VLg++85lMdIMjtSg4jMi7Mz0LixWGTv3wOXLsWNqjp7FrhwAQgPBw4dEovMygqoUMGwFLByZZHEotyLiSkiIiIiypiTJ8UZiasrUK6c0tGkrF07cUb18CGwbx/QvLnSEeUYWTFSg4jMh60tUKOGWGSxsaJyOv7IqrNnxYgr+fLChWJblUq0KExYCpg/vyIPhxTAxBQRERERZYw59JeS2dgAPXoA8+aJcj4mpoiIsoyFhRgZVaEC0KePWCdJwL17iZNVjx+LJNb168CqVXH78PBInKzy8DD9fzeUfkxMEREREVHGmEN/qfj8/ERiav164I8/AEdHpSMiIso1VCqgWDGxxO9B9/SpYYP1s2eBW7eABw/Esnlz3Lb58sX1rZJLAUuVYnW2uWNiioiIiIjSLyYGOHJEXI7fbMSU1aol6kVu3BBdu/v3VzoiIqJcr0ABMT+Fr2/curAw4Px5w5FVly8DL14Ae/aIRWZnB1SpYjiyqmJFwNo6+x8LZQwTU0RERESUfqdPA+/eia+vy5dXOpq0UanEqKlvvxXlfExMERGZJCcnoEEDsciiouKarMvL+fNARISYh0OeiwOIKyWMn6yqUkXsl0wPE1NERERElH5yf6mGDQG1WtFQ0uWjj4Bx40QZ4t27oqaEiIhMnrU14OMjFplWC9y8mbgU8OVLkbQ6fx7w94/bvmRJwzJAb28xYouUxcQUEREREaWfufWXknl4AM2aAbt3A0uWABMmKB0RERFlkEYDlC0rlp49xTpJEr2pEjZZf/BA9K66dQtYuzZuH4UKJW6y7uXFJuvZiYkpIiIiIkqf2Fjg0CFx2Vz6S8Xn5xeXmBo/nmcfREQ5iEoFFC0qlg4d4tY/f544WXXjhpgV8PFjYOvWuG2dnRMnq8qUESWCZHx8WomIiIgofc6cAcLDgbx5gUqVlI4m/Tp1AhwcgDt3RIItfhMTIiLKkfLnB1q0EIssPBy4cMGwFPDSJeD1a2DfPrHIbGyAypUNSwErVRLrKXOYmCIiIiKi9JHL+Bo0MK/+UjJ7e6BrV2DRItEEnYkpIqJcycEBqFtXLLLoaODKFcORVefOiSTWiRNikWk0QLlyhiOrqlYVI67SS6sFgoKAkBDA3V38a9JoMvkAzQQTU0RERESUPubaXyo+Pz+RmFqzBvj1VzHfOBER5XpWViK5VLVq3OStOh1w+7Zhg/WzZ4Fnz8QIq0uXgKVL4/ZRvHjiUkB39+Tvc/16YPhw4OHDuHVFigBz5gCdO2fFozQtTEwRERERUdrJX+kC5tlfStaggehue+8esHEj0KuXwgEREZGpUquBUqXE0q2bWCdJojdV/JFVZ84AwcGiUvzOHWDdurh9FCwYl6SSSwGLFwc2bAC6dBH7i+/RI7E+ICDnJ6eYmCIiIiKitDt3DggLA/LkAapUUTqajFOrgb59gcmTxVziTEwREVE6qFRA4cJiads2bv3Ll+JfZfyE1bVrQGgosGOHWGSOjkBUVOKkFCDWqVTAiBGiiXtOLutjYoqIiIiI0k4u46tf3/w/JcuJqd27Rf1EkSJKR0RERGbOxQVo2lQssogI4OJFwzLAixeBt29T3pckAQ8eiIHK5jxIOTVm2K2SiIiIiBSTE/pLyUqUECV9kgQsW6Z0NERElEPZ2QG1agGffgr89Rdw8qRISv30U9puHxKStfEpjYkpIiIiIkobnS5n9JeKz89P/Fy8OOlaCiIioixgaSmSVWmRUuP0nICJKSIiIiJKmwsXgFevRFMMb2+lozGOrl0BW1vRAOTkSaWjISKiXKRBA1FFrlIlfb1KBXh4iO1yMiamiIiIiCht5DK+evUAixzSqtTJCejUSVxevFjZWIiIKFfRaIA5c8TlhMkp+ffZs82/pWNqmJgiIiIiorTJSf2l4pPL+VauFNMjERERZZPOnYGAADG7X3xFioj1nTsrE1d2yiFfdRERERFRltLpgIMHxeWc0l9K1qyZOCN49Aj491+gSxelIyIiolykc2egQwfRxjEkRPSUatAg54+UknHEFBERERGl7vJl4MULwN4e8PFROhrj0miAjz4Sl1nOR0RECtBoxPc+PXuKn7klKQUwMUVEREREaSGX8dWtK6YSymnkcr7t24HQUGVjISIiykWYmCIiIiKi1OXU/lKysmWBmjUBrRZYsULpaIiIiHINJqaIiIiIKGWSFJeYymn9peKTR02xnI+IiCjbMDFFRERERCm7ehV49gywtQVq1FA6mqzTowdgZQWcPy8WIiIiynJMTBERERFRyuTRUnXqiMRNTuXiArRrJy5z1BQREVG2YGKKiIiIiFKWG8r4ZHI537JlQEyMsrEQERHlAkxMEREREVHy4veXyqmNz+Nr1QooUECULu7YoXQ0REREOR4TU0RERESUvBs3gCdPAGtrMWtdTmdpCfTuLS6znI+IiCjLMTFlrrRaqA4cQOGDB6E6cEBMbUxERERkbPJoqdq1ARsbZWPJLnI537//Ai9fKhsLERFRDsfElDlavx7w8oJFixaoPnMmLFq0ALy8xHoiIiIiY8pN/aVkVaqIJToaWLVK6WiIiIhyNCamzM369UCXLsDDh4brHz0S65mcIiIiImORJGD/fnE5N/SXik8eNcVyPiIioizFxJQ50WqB4cPFh8SE5HUjRrCsj4iIiIzj9m3g8WPAykqU8uUmvXoBGg1w4gRw9arS0RAREeVYTEyZk6CgxCOl4pMk4MEDsR0RERFRZsllfDVrAra2ysaS3QoWBFq3Fpc5aoqIiCjLMDFlTkJCjLsdERERUUpyY3+p+Pr1Ez+XLuWIdCIioizCxJQ5cXc37nZEREREycnN/aVkbdsCefOKcsY9e5SOhoiIKEdiYsqcNGgAFCkCqFQpbxcUBOh02RMTERER5Uz37okWARYWQJ06SkejDGtroGdPcZnlfERERFmCiSlzotEAc+aIywmTU/F/Hz8e6NQJePMm+2IjIiKinEUu46tRA7C3VzYWJcmz823YAISFKRsLERFRDsTElLnp3BkICAAKFzZcX6QIsG4d8Pff4tu9zZuB6tWBS5eUiZOIiIjMW27vLyWrUQMoWxZ4/x5Yu1bpaIiIiHIcJqbMUefOwL17iA0MxKlRoxAbGAjcvSvWDxwIHDoEFC0K3LoF1KoFrFypdMRERERkbnJ7fymZShU3asrfX9FQiIiIciImpsyVRgOpUSM8atgQUqNGosxPVr06cPo00KIFEBEB9OoFjBgBxMQoFi4RERGZkfv3RY8pjQaoW1fpaJTXp49IUB06BNy+rXQ0REREOQoTUzlV/vzA9u3AN9+I3+fMAZo2BUJClI2LiIiITJ9cxufjAzg6KhuLKShSRHzhBwBLligbCxERUQ7DxFROptEAU6aIZp1OTuJbPh8f4PBhpSMjIiIiU8b+UonJ5XxLlnD2YyIiIiNiYio36NgROHkSqFBBjJhq3Bj49VdAkpSOjIiIiEwR+0sl1rGjGD127x4QFKR0NERERDkGE1O5RenSwLFjQPfuQGwsMHy46Jfw7p3SkREREZEpefRI9FFSq4H69ZWOxnTY2QHduonLixcrGwsREVEOwsRUbuLgIGbomzVLlPmtWAHUqSNm7yMiIiIC4sr4vL1FKwCKI5fzrV3LL/eIiIiMhImp3EalEjP07d0LFCwIXLwoZvH791+lIyMiIiJTwP5SyatfHyheHAgPB9avVzoaIiKiHIGJqdyqYUPgzBkxBfSbN0D79sB33wFardKRERERkZLYXyp5KhXQt6+4zHI+IiIio2BiKjcrVAjYtw8YOlT8/sMPwAcfAC9fKhsXERERKSMkBLhxQyRgGjRQOhrTJCem9u4FHjxQNhYiIqIcgImp3M7KCvjtN2DpUsDWFti5E/DxEaOpiIiIKHc5eFD8rFIFcHZWNBSTVayYGE0mSeLzExEREWUKE1Mk9OkDHD0q+ibcuwfUqwf4+ysdFREREWUn9pdKG7kJ+uLFIkFFREREGcbEFMWpUgU4dQpo2xaIjAT69wf+9z8gKkrpyIiIiCg7sL9U2nTpAtjZibLH48eVjoaIiMisMTFFhvLmBTZtAiZNEv0l5s8XH04fPlQ6MiIiIspKT58CV6+Ky+wvlTJHR6BzZ3GZTdCJiIgyhYkpSkytBsaPB7ZuFYmq48eBatVEk08iIiLKmeT+UpUrA/nyKRuLOZDL+VatEiPNiYiIKEOYmKLktW4tSvuqVgWePQNatACmTWMvBSIiopyIZXzp06QJ4OEBvH4NbN6sdDRERERmi4kpSlnx4sCRI+JbQZ0OGDMG6NoVePtW6ciIiIjImOTG50xMpY1GA3z0kbjMcj4iIqIMM4nE1O+//w4vLy/Y2NigVq1aOHHiRJput2rVKqhUKnTs2DFrA8ztbG2BRYuAP/4ALC2BdeuAmjXj+lAQERGReXv+HLh0SVxu2FDZWMxJ377i586dwJMnysZCRERkphRPTK1evRqjRo3ChAkTcObMGVSpUgW+vr54+vRpire7d+8eRo8ejQZszpk9VCrg009F/4nChYFr10RyKiBA6ciIiIgos4KCxM8KFQBXV2VjMSdlygC1awNaLbB8udLREBERmSXFE1MzZ87EoEGD0L9/f5QvXx7z5s2DnZ0dFi5cmOxttFotevfujUmTJqF48eLZGC2hdm3gzBmgcWMgPFyU9Y0ZA8TGKh0ZERERZRT7S2Wc3AR98WL24SQiIsoARRNT0dHROH36NJo3b65fp1ar0bx5cxw9ejTZ202ePBkFChTAgAEDsiNMSqhAASAwEBg9Wvw+bRrQsqWYZpqIiIjMD/tLZVz37oC1NXDxInDunNLREBERmR0LJe/8+fPn0Gq1KFiwoMH6ggUL4tq1a0ne5tChQ/jnn39wLo3/+KOiohAVFaX/PSwsDAAQExODmJiYjAVuIuT4FXscP/4IlY8PNIMGQbVvHyQfH2hXrYJUs6Yy8RApQPHjkIh4HGbWq1ewuHABKgAxdeoAfB7Tx8EBmrZtoV63DtqFC6GbOVPpiBTB45BIeTwOyZSk53WoaGIqvd6+fYuPPvoIf//9N/Lnz5+m20ydOhWTJk1KtH7Xrl2ws7MzdoiKCAwMVO7ObW3hMHUqav70ExwfPoS6cWNcHDgQ93x9RV8qolxC0eOQiADwOMwotxMnUEuS8LZIEew9c0bpcMxSgXLlUAdA7JIl2NmoESRLS6VDUgyPQyLl8TgkUxAREZHmbVWSpFwxfHR0NOzs7BAQEGAws56fnx9ev36NTZs2GWx/7tw5eHt7Q6PR6NfpdDoAogTw+vXrKFGihMFtkhox5eHhgefPn8PJySkLHlX2iYmJQWBgIFq0aAFLpT8AhYVBM2gQ1Bs2AAB0H30E7dy5YkY/ohzMpI5DolyKx2HmqL/8Epo5c6AdNAi6339XOhzzFBsLi+LFoXryBLEBAZDat1c6omzH45BIeTwOyZSEhYUhf/78ePPmTaq5F0VHTFlZWcHHxwd79uzRJ6Z0Oh327NmDoUOHJtq+bNmyuHjxosG6cePG4e3bt5gzZw48PDwS3cba2hrW1taJ1ltaWuaYg9UkHku+fMC6daLf1NixUC9dCvWlS2JdsWLKxkaUDUziOCTK5XgcZtB/M/JpmjSBhs9fxlhaAr17AzNmwGL5cuDDD5WOSDE8DomUx+OQTEF6XoOKz8o3atQo/P3331i8eDGuXr2KTz/9FO/evUP//v0BAH379sXYsWMBADY2NqhYsaLB4uzsDEdHR1SsWBFWVlZKPhRSqcQMfYGBQP78wNmzgI8PsGOH0pERERFRUt68iWvYzcbnmSPPzrdlC/DihbKxEBERmRHFE1Pdu3fH9OnTMX78eFStWhXnzp3Djh079A3R79+/j5CQEIWjpHRp2hQ4cwaoWRN49Qpo0wb4/nvgv7JLIiIiMhGHDon/z6VKAYUKKR2NeatUCfD2Fs3jV65UOhoiIiKzoXhiCgCGDh2K4OBgREVF4fjx46hVq5b+uv3798Pf3z/Z2/r7+2Pjxo1ZHySlj4cHcPAg8MkngCQB48cDHToAr18rHRkRERHJ9u8XPzlayjjkUVOLFysbBxERkRkxicQU5VDW1sC8ecDCheLyli1A9erAhQtKR0ZEREQAcOCA+MnElHH06gVYWACnTgGXLysdDRERkVlgYoqyXv/+wJEjgKcncPs2ULs2sHy50lERERHlbm/fitJ7gIkpY3F1BT74QFzmqCkiIqI0YWKKske1asDp04CvL/D+PdCnDzBsGBAdrXRkREREudPhw4BWCxQvLkrwyTjkcr5ly4DYWGVjISIiMgNMTFH2yZcP2LoVGDdO/P7bb6JR+uPHysZFRESUG7G/VNb44APxmSckBNi9W+loiIiITB4TU5S9NBoxQ9+mTYCTk/i21scHCApSOjIiIqLchf2lsoaVFdCzp7jMcj4iIqJUMTFFymjfXjQGrVgRePIEaNIEmD1bzOBHREREWevdO/F/GGBiKivI5XwbNwJv3igaChERkaljYoqUU6oUcOyYmMFGqwVGjhSX371TOjIiIqKc7cgR0f/I0xPw8lI6mpzHxwcoXx6IjATWrFE6GiIiIpPGxBQpy95eNAedM0dMr7xqlZi17+ZNpSMjIiLKudhfKmupVHGjpvz9FQ2FiIjI1DExRcpTqcQMffv2AW5uwKVLQPXqog8VERERGR/7S2W9Pn0AtVqMTuMXbkRERMliYopMR/36wJkz4mdYGNCxI/Dtt6LMj4iIiIwjIgI4cUJcbtxY0VBytEKFgJYtxeUlS5SNhYiIyIQxMUWmxd0d2LsXGD5c/P7jj0CbNsCLF8rGRURElFMcPQrExABFigDFiikdTc4ml/MtWQLodMrGQkREZKKYmCLTY2kpZuhbvhywswN27RJNRE+fVjoyIiIi8xe/jE+lUjaWnK5DByBPHuD+/bjnnYiIiAwwMUWmq1cvMWtfyZJAcDBQrx7wzz9KR0VERGTe2F8q+9jaAt26icuLFysbCxERkYliYopMW6VKwMmTQLt2QFQUMHAgMHiwuExERETpExkJHD8uLrO/VPaQy/kCAoDwcGVjISIiMkFMTJHpc3YGNm4EfvhBlBz8/TfQoIEYFk9ERERpd+yY+HLH3V2MSKasV7eueK7fvQPWrVM6GiIiIpPDxBSZB7VazNC3fTvg4iJGUfn4AHv2KB0ZERGR+WB/qeynUsWNmmI5HxERUSJMTJF58fUVTdCrVQOePxfTMP/0EyBJSkdGRERk+thfShkffSR+7tsn+mYSERGRHhNTZH68vIBDh4D+/cXUy2PHAh9+CISFKR0ZERGR6YqKAo4eFZfZXyp7eXoCTZqIy0uXKhsLERGRiWFiisyTra2YoW/+fMDKCtiwAahRA7hyRenIiIiITNOJE6L5ecGCQJkySkeT+8jlfEuWcKQ3ERFRPExMkflSqcQMfUFBQJEiwI0bQM2awJo1SkdGRERkeuQyvoYN2V9KCR9+CNjbAzdvxo1cIyIiIiamKAeoWRM4cwZo2lTMeNO9O/DFF0BsrNKRERERmQ72l1KWg4NITgFsgk5ERBQPE1OUM7i6Ajt3AmPGiN9nzgSaNwdCQ5WNi4iIyBRERwNHjojL7C+lHLmcb9Uq4P17ZWMhIiIyEUxMUc5hYQH8/DMQECC+lTxwQMzex+HyRESU2506BUREAPnzA+XLKx1N7tW4MVC0qJiwZdMmpaMhIiIyCUxMUc7z4YfAyZNAuXLA48eiZOGPP9holIiIci/2lzINajXQt6+4zHI+IiIiAExMUU5Vtixw/DjQtSsQEwMMGSKGz0dEKB0ZERFR9mN/KdMhJ6Z27RJfoBEREeVyTExRzuXoCKxeDUyfDmg0wNKlQN26wJ07SkdGRESUfWJigMOHxWX2l1JeqVLi84hOByxfrnQ0REREimNiinI2lUrM0BcYKBqknz8P+PgA27YpHRkREVH2OHMGCA8HXFyAihWVjoaAuCboixez1QAREeV6TExR7tCkifhgXrs28Po10LYtMGmS+LaSiIgoJ5PL+Bo0ED2OSHndugHW1sDly+LzCRERUS7GTyeUexQpAuzfD3z2mfh2cuJEoF074NUrpSMjIiLKOuwvZXqcnYGOHcVlf38FAyEiIlIeE1OUu1hbA7//LobO29iIkr7q1YFz55SOjIiIyPhiY4FDh8Rl9pcyLf36iZ8rVwLR0YqGQkREpCQmpih36tsXOHIEKFZMNEOvU0c0Ryei3EWrFSMpV64UP7VapSMiMq5z54CwMCBPHqByZaWjofhatADc3YEXL4CtW5WOhoiISDFMTFHu5e0NnDoFtG4NREaKZNWQIfzWkii3WL8e8PISPeh69RI/vbzEeqKcIn5/KY1G2VjIkEYD9OkjLi9erGwsRERECmJiinI3FxdgyxZgwgTx+x9/iFKHR48UDYuIstj69UCXLsDDh4brHz0S65mcopxCTkyxjM80ybPzbd0KPHumbCxEREQKYWKKSK0WjdC3bBHNSI8eBapVi/swT0Q5i1YLDB+e9BTt8roRI1jWR+ZPqwUOHhSX2fjcNFWoAPj4iF5gK1cqHQ0REZEimJgikn3wgSjtq1wZePoUaNYMmDkz6ZNXIjJfQUGJR0rFJ0nAgwdiOyJzduEC8OYN4OgIVK2qdDSUHHnUFMv5iIgol2Jiiii+EiXEiKk+fcQ3zV98AfToAYSHKx0ZERlLSIhxtyMyVfLI3/r1AQsLZWOh5PXsCVhaAmfOABcvKh0NERFRtmNiiighOztgyRLgt9/EB/k1a4BatYDr15WOjIiMwd3duNsRmSr2lzIP+fMDbduKyxw1RUREuRATU0RJUamAoUPFh3p3d+DKFaBGDWDDBqUjI6LMatAAKFIk+etVKsDDQ2xHZK50OvaXMidyOd+yZaLfFBERUS7CxBRRSurWFUPrGzYE3r4FOncGxo5lU2Qic6bRAIMHp7zN7NliOyJzdekS8PIlYG8vJvQg09a6tRg5FRoK7NqldDRERETZiokpotS4uQG7dwOjRonff/oJaNWK0zoTmSutFli3Tly2t098/W+/iSQ0kTmTy/jq1RP9i8i0WVkBvXqJyyznIyKiXIaJKaK0sLQEZswAVq0SJ7K7d4vpnU+eVDoyIkqvf/4Bzp8HnJ2B27eBffuAFStEuS4gZuQjMnfsL2V+5HK+TZuAV6+UjYWIiCgbMTFFlB7duwPHjwOlSomT1/r1gb//VjoqIkqr16+Bb78VlydNAgoWFCfuPXsCX38t1vv7AzExCgVIZASSFJeYYn8p8+HtDVSsCERFiYlXiIiIcgkmpojSq0IFMVKqY0cgOlr0qhk4EIiMVDoyIkrN5MnA8+dA+fLAp58aXteunUhUhYYC//6rTHxExnDlinid29oC1asrHQ2llUoVN2rK31/RUIiIiLITE1NEGZEnj+hRM3UqoFaL0qD69YHgYKUjI6LkXL0q+kcBorl5wr47lpZAv37iMkdCkjmTR0vVrSt6F5H56N1bTLxw7Bhw/brS0RAREWULJqaIMkqtFqU/O3cC+fIBp0+LvlOBgUpHRkQJSRIwcqSYhr19e6BFi6S3GzhQ/Ny5k4lmMl/sL2W+3N0BX19xeckSZWMhIiLKJkxMEWVW8+YiKVW9OvDihfhA+eOPgE6ndGREJNu2TSSb5IkMklOyJNCkiUhkLVyYffERGYskAfv3i8vsL2We5HK+pUv5WYKIiHIFJqaIjMHTEwgKEqMtJEk0V+7cGXjzRunIiCg6WoyWAsTPkiVT3n7QIPFz4UJAq83a2IiM7fp14OlTwMYGqFlT6WgoI9q3F7OGPnggZg0lIiLK4ZiYIjIWGxvRl+bvv0VPj02bxPTzly4pHRlR7vbbb8DNm4CbGzBuXOrbd+okynMfPgR27Mj6+IiMSS7jq10bsLZWNhbKGBsbMQswACxerGwsRERE2YCJKSJjGzgQOHQIKFpUnAzXqgWsXq10VES5U2iomIkPEJMVODqmfhsbG6BvX3H5r7+yLjairMD+UjmDXM63bh3w9q2ysRAREWUxJqaIskKNGqLvVPPmQEQE0KMHMGoUEBOjdGREucu33wJhYaIHnJxsSgu5nG/rVuDx46yJjcjY2F8q56hdGyhVSnyGCAhQOhoiIqIsxcQUUVbJn1+UAY0dK36fNQto1gx48kT8rtWKE4iVK8VP9rIhMq7Tp+MamP/6q5hJM63KlQPq1RPH5aJFWRMfkbHdugWEhIhy8lq1lI6GMkOlAvr1E5dZzkdERDkcE1NEWUmjETP0bdggSoiCgoBq1cQ6Ly8x+1evXuKnlxewfr3SERPlDJIEDB8ufvbuDdSpk/59yKOm/vmHM2OReZDL+GrVAmxtlY2FMu+jj0SC6sAB4O5dpaMhIiLKMkxMEWWHjh2BU6eA8uXFt9nffisaK8f36BHQpQuTU0TGsHo1cPgwYGcH/PRTxvbRtSuQJ484Idyzx7jxEWUF9pfKWTw8gKZNxeWlS5WNhYiIKAsxMUWUXUqXBo4cSf5bbEkSP0eMYFkfUWZERABffikuf/MNUKRIxvZjZydGWwFitk0iU8b+UjmT3AR9yZK4zwlEREQ5DBNTRNnp7Fng/fvkr5ck4MEDUfJHRBnzyy9iRKKXl5h0IDPkcr6NG4GnTzMbGVHWuXtXvO4tLTNWukqmqXNnwMEBuH1bjAIlIiLKgZiYIspOISHG3Y6IDAUHAz//LC5Pn575PjtVq4pZNmNi2ICYTJtcxlezphjtRzmDvb0o8wcAf39FQyEiIsoqTEwRZSd3d+NuR0SGxowBIiNFj53OnY2zT3nU1IIFLKUh0yUnpljGl/PI5Xxr1ohSZSIiohyGiSmi7NSggeh3o1Ilfb1KJZqdNmiQvXER5QQHD4oTN7UamD07+eMsvXr0EKMWbtwQ90FkithfKudq2FCUJr99K8qKiYiIchgmpoiyk0YDzJkjLid10ixJ4oRao8nWsIjMnlYLDB8uLg8eDFSpYrx9OzoCPXuKy2yCTqYoOFgsGg1Qt67S0ZCxqdVA377iMkuKiYgoB2Jiiii7de4MBAQAhQsnvk6tBkqUyP6YiMzdwoXAuXOAszPw/ffG379czhcQALx8afz9E2WGXMZXo4ZolE05j5yY2r0bePRI2ViIiIiMjIkpIiV07gzcuwfs2wesWCF+du4M6HTAgAFAbKzSERKZj9evgW+/FZcnTQLy5zf+fdSoAVSuDERFAcuWGX//RJnBMr6cr0QJoH598TmB70FERJTDMDFFpBSNRjRo7tlT/Pz9dzHa4/RpUc5HRGkzeTLw7BlQrhzw6adZcx8qVdyoqb//ZhN0Mi1sfJ47yE3QFy/mexAREeUo6U5MeXl5YfLkybh//35WxEOUe7m5ATNnisvffQfcuqVsPETm4No14LffxOVZswBLy6y7r969ARsb4NIl4PjxrLsfovR4+BC4c0eUgterp3Q0lJW6dhXvQVevAidPKh0NERGR0aQ7MTVixAisX78exYsXR4sWLbBq1SpERUVlRWxEuU+/fkDz5mK6+0GD+I0oUWpGjRKlr+3aAb6+WXtfefMC3bqJy3/9lbX3RZRW8mgpHx/AyUnZWChr5ckDdOokLrMJOhER5SAZSkydO3cOJ06cQLly5fD555/D3d0dQ4cOxZkzZ7IiRqLcQ6UC5s8H7OxEz5AFC5SOiMh0bdsGbN8uRknNmJE99ymX861eDYSFZc99EqWE/aVyl379xM+VK0XPOyIiohwgwz2mqlWrhl9//RWPHz/GhAkTsGDBAtSoUQNVq1bFwoULIXGkB1HGFC8O/PCDuDx6NGffIUpKdDQwcqS4PHIkUKpU9txvvXqil1VEhJi4gEhp7C+VuzRrJmb1ffUK2LJF6WiIiIiMIsOJqZiYGKxZswbt27fHF198gerVq2PBggX48MMP8c0336B3797GjJModxk2DKhZU4zIGDKEJX1ECc2dC9y4ARQsGDcjX3ZQqYCBA8Xlv//OvvslSkpICHDzpnhd1q+vdDSUHTQaoE8fcZnlfERElEOkOzF15swZg/K9ChUq4NKlSzh06BD69++P7777Drt378aGDRuyIl6i3EGjAf75R5QobdoEBAQoHRGR6QgNBSZNEpenTs3+vjp9+wJWVsCZM2IhUoo8WsrbW8zqSrmDPDvf9u3A06fKxkJERGQE6U5M1ahRAzdv3sSff/6JR48eYfr06ShbtqzBNsWKFUOPHj2MFiRRrlSxIvDNN+Ly0KHAixfKxkNkKsaNE6MJfXziTtCyU/78cQ2IOWqKlMT+UrlTuXJAjRpi4geWFBMRUQ6Q7sTUnTt3sGPHDnTt2hWWyUzLbW9vj0WLFmU6OKJcb+xYoHx58Y3oF18oHQ2R8s6cEaMJAeDXXwF1hivSM0dugr58OfDunTIxELG/VO4lJ+X9/RUNg4iIyBjS/Yn+6dOnOH78eKL1x48fx6lTp4wSFBH9x9panISrVKKXxM6dSkdEpBxJAoYPFz979QLq1lUuliZNgBIlgLdvxQx9RNktNBS4dk38f2jQQOloKLv16CHK/c+fFwsREZEZS3diasiQIXjw4EGi9Y8ePcKQIUMyFMTvv/8OLy8v2NjYoFatWjhx4kSy265fvx7Vq1eHs7Mz7O3tUbVqVSxdujRD90tkFmrXFs3QAeCTT4DwcGXjIVLKmjXAoUOAnR3w88/KxqJWswk6KevgQfGzcmXAxUXZWCj75csHtG8vLrMJOhERmbl0J6auXLmCatWqJVrv7e2NK1eupDuA1atXY9SoUZgwYQLOnDmDKlWqwNfXF0+Taebo4uKCb7/9FkePHsWFCxfQv39/9O/fHzs5koRysh9+ALy8gODg7J2BjMhUREQAX34pLo8dCxQpomw8ANCvH2BhARw7Bly6pHQ0lNuwvxTJ5XzLlwMxMcrGQkRElAnpTkxZW1sjNDQ00fqQkBBYWFikO4CZM2di0KBB6N+/P8qXL4958+bBzs4OCxcuTHL7xo0bo1OnTihXrhxKlCiB4cOHo3Llyjh06FC675vIbDg4APPni8u//QYcPapsPETZ7ZdfgAcPAE9P0+m35uYGtGsnLnPUFGU39peiVq0AV1fRh5Jf0BIRkRlLd2KqZcuWGDt2LN68eaNf9/r1a3zzzTdo0aJFuvYVHR2N06dPo3nz5nEBqdVo3rw5jqbhxFuSJOzZswfXr19Hw4YN03XfRGanZUsxQkOSgAEDgKgopSMiyh7378eV7k2fDtjaKhtPfHIT9KVLgchIZWOh3OP5c+DyZXGZn39yL0tLoHdvcZnlfEREZMbSPcRp+vTpaNiwITw9PeHt7Q0AOHfuHAoWLJjuXk/Pnz+HVqtFwYIFDdYXLFgQ165dS/Z2b968QeHChREVFQWNRoM//vgj2aRYVFQUouKdwIeFhQEAYmJiEGPmw57l+M39cVA6/PQTLLZvh+rqVWi//x66CROUjijX43GY9TSjR0MdGQldw4bQtm9vWiUrTZrAomhRqO7fR+zq1ZB69VI6olwptx2Hqr17YQFAqlABsXnymNYxQdmrVy9Yzp4NafNmxIaGKtpvLLcdh0SmiMchmZL0vA7TnZgqXLgwLly4gOXLl+P8+fOwtbVF//790bNnT1haWqZ3dxni6OiIc+fOITw8HHv27MGoUaNQvHhxNG7cONG2U6dOxaRJkxKt37VrF+zs7LIh2qwXGBiodAiUjdz9/FDzl1+g+uknBLm64q2Xl9IhEXgcZhWXy5fRYO1aSGo1DnTqhLDt25UOKZEydeui7P37eD1tGg47OysdTq6WW47DSkuWoDiAu56euLhtm9LhkMIae3khz717uDJ+PO61aaN0OLnmOCQyZTwOyRRERESkeVuVJElSFsaSoujoaNjZ2SEgIAAdO3bUr/fz88Pr16+xadOmNO1n4MCBePDgQZIN0JMaMeXh4YHnz5/Dyckp049BSTExMQgMDESLFi2yLSlIJkCSoOnaFerNm6GrXh3aoCBAo1E6qlyLx2EW0mphUacOVOfOQTt4MHRz5yodUdIePIBFqVJQ6XSIuXQJKF1a6Yhyndx2HFr4+EB18SJiV6yA1KWL0uGQwtSzZ0MzZgx0NWpAe/iwYnHktuOQyBTxOCRTEhYWhvz58+PNmzep5l7S3638P1euXMH9+/cRHR1tsL69PHVtGlhZWcHHxwd79uzRJ6Z0Oh327NmDoUOHpnk/Op3OIPkUn7W1NaytrROtt7S0zDEHa056LJRGf/4JHDgA9alTUP/xBzBqlNIR5Xo8DrOAvz9w7hzg7AzNDz9AY6rPb/HiQOvWwNatsPT3B6ZNUzqiXCtXHIcvX+pngbRo2lT0GaLcrW9fYOxYqE+ehPr2baBsWUXDyRXHIZGJ43FIpiA9r8F0J6bu3LmDTp064eLFi1CpVJAHXKlUKgCAVqtN1/5GjRoFPz8/VK9eHTVr1sTs2bPx7t079O/fHwDQt29fFC5cGFOnTgUgSvOqV6+OEiVKICoqCtu2bcPSpUvx559/pvehEJmvQoVEE+hBg4Bx44COHcXJMVFO8fo18O234vLEiWLmKVM2aBCwdatoQDxlCmBlpXRElFMFBYlJMMqVAxL06KRcqmBBkRzfskW8B/33mZmIiMhcpHtWvuHDh6NYsWJ4+vQp7OzscPnyZRw8eBDVq1fH/v370x1A9+7dMX36dIwfPx5Vq1bFuXPnsGPHDn1D9Pv37yMkJES//bt37/DZZ5+hQoUKqFevHtatW4dly5Zh4MCB6b5vIrM2YADQpAnw/r04KVauKpfI+L7/Hnj2THzz/9lnSkeTug8+ANzdRcxpLEMnyhD5s1ajRoqGQSbGz0/8XLoUSOeXxEREREpLd2Lq6NGjmDx5MvLnzw+1Wg21Wo369etj6tSpGDZsWIaCGDp0KIKDgxEVFYXjx4+jVq1a+uv2798Pf39//e8//PADbt68iffv3+Ply5c4cuQIunfvnqH7JTJrKhXw99+ArS2wdy+waJHSEREZx/XrwK+/isuzZ5tHqZKFBfDfSF/8/beysVDOduCA+MnEFMXXrh2QNy/w6JH4TEBERGRG0p2Y0mq1cHR0BADkz58fjx8/BgB4enri+vXrxo2OiFJWooQYWQKIPlPxRhcSma1Ro4DYWKBtW8DXV+lo0m7AAPEzMBC4e1fZWChnev1a9F0DmJgiQ9bWQI8e4vLixcrGQkRElE7pTkxVrFgR58+fBwDUqlULv/zyCw4fPozJkyejOHvcEGW/4cOB6tWBN2+AdEwaQGSStm0Ti6UlMHOm0tGkT/HiQPPm4vI//ygbC+VMcn+p0qVF6ShRfHI53/r1QFiYsrEQERGlQ7oTU+PGjYNOpwMATJ48GXfv3kWDBg2wbds2/CqXXhBR9rGwECfBFhbiw+i6dUpHRJQx0dHAyJHi8ogRQKlSioaTIYMGiZ8LF4pRX0TGxDI+SknNmkCZMqL35Nq1SkdDRESUZulOTPn6+qJz584AgJIlS+LatWt4/vw5nj59iqZNmxo9QCJKg8qVga+/FpeHDAFevVI2HqKMmDsXuHEDKFBAzDZpjjp0APLnF2W1W7cqHQ3lNExMUUpUKqBfP3GZ5XxERGRG0pWYiomJgYWFBS5dumSw3sXFBSqVyqiBEVE6jRsnZjALDQW++ELpaIjS5+lTYNIkcXnqVMDJSdl4MsraOu7EkE3QyZjCwoAzZ8RlJqYoOX36iARVUBBw547S0RAREaVJuhJTlpaWKFq0KLSchpbI9Fhbi5I+lUrM0Ld7t9IREaXduHHixNvHJy6xY64GDhQ/t28HHj5UNhbKOQ4dAnQ6MelFkSJKR0OmqkiRuF53S5YoGwsREVEapbuU79tvv8U333yDly9fZkU8RJQZdevGNUAfNAh4907ZeIjS4uxZYMECcXnOHECd7n9NpqVMGaBhQ5FEWLhQ6Wgop2AZH6WV3AR9yRLxPkRERGTi0v3pf+7cuTh48CAKFSqEMmXKoFq1agYLESnsxx+BokWBe/eA775TOhqilEmSmFlSkoBevYB69ZSOyDjkJuj//ANwlDEZAxNTlFadOgGOjsDdu2KkHRERkYmzSO8NOnbsmAVhEJHRODgA8+cDrVsDs2cD3bsDtWopHRVR0tasEb1Q7OyAn39WOhrj+fBD4PPPgfv3gcBAoFUrpSMicxYeDpw6JS4zMUWpsbMDunYVIzb9/cUITiIiIhOW7sTUhAkTsiIOIjKmVq2Ajz4Cli4FBgwQDXOtrJSOishQRATw5Zfi8tdf56y+Oba24hj87TfRBJ2JKcqMw4fFyDsvL8DTU+loyBz4+YnE1Nq14n3I3l7piIiIiJJl5o08iChZs2YBrq7A5ctiljMiUzNtGvDggSg9HT1a6WiMTy7n27wZePJE2VjIvLGMj9Krfn2geHEx2m7DBqWjISIiSlG6E1NqtRoajSbZhYhMRL584ltSAJgyRSSoiEzF/ftxpXvTp4sRRjlNpUpA7dpAbKwopyHKKCamKL3UaqBvX3F58WJlYyEiIkpFuhNTGzZswPr16/XL6tWr8fXXX8Pd3R1//fVXVsRIRBnVrRvQvj0QEyNK+tiEmUzFV1/h/+3deZzNZf/H8feZ3b5kX4pKpDD2PSlEUm6RtIiYJEKTQneWboUkjSKyphAlqtuNmixZszaVQlHZDVqMdYyZ8/vj+h1jGMxwzrnO8no+Hudxrvme75x5T83XnPmc6/pcOnXK/KHdtq3tNJ7jmjU1eTK7Y+HqnDghbdhgxnfeaTUK/IyrMLVkiZmdCgCAj8p2YeqBBx7IcGvbtq1ee+01jRw5Ul988YUnMgK4Wg6H9O67Ut680rp10tixthMBptn57NnmHf24OPNzGqjatze7Y+3cKS1fbjsN/NHatebNhdKlTY8pIKvKljWNz51OacYM22kAALgkt/WYqlOnjpYsWeKupwPgLiVLml4+kvTSS2b7aMCW1FSpd28zjomRoqOtxvG4XLmkRx4x40mT7GaBfzp/GV8gF3HhGU88Ye6nTzcFKgAAfJBbClOnTp3S22+/rZIlS7rj6QC4W9eu5o+akyelbt14cQp7pk2TvvtOypdPGjrUdhrvcC3nmzdPOnLEbhb4H/pL4Vq0bWt6+G3fbmZOAwDgg7JdmCpQoIAKFix47lagQAHlyZNHU6dO1RuuWRkAfEtIiJmtERUlxcfTCBV2HD1qZu1J0pAhZtfIYFC9ulS1qnTmjPThh7bTwJ+cOpVeTKC/FK5G3rxSmzZmzO9+AICPCsvuJ7z11ltynDeVPCQkRIULF1bt2rVVoEABt4YD4EblykmvvGKaTsfGSs2bS8WK2U6FYDJ0qHT4sFShgtSjh+003hUTIz3zjCkQ9+nDkixkzbffmoJmiRLSTTfZTgN/1amTNHOm6e331lvmTSoAAHxItgtTnTp18kAMAF4RGyvNmSNt3iw9+6z0ySe2EyFYbN8ujRljxm+9JYWH283jbY88IvXtK23dKq1ZI9WvbzsR/AH9peAOjRtLpUpJe/dK//2v1K6d7UQAAGSQ7aV806ZN0yeZ/DH7ySefaDpThAHfFhYmTZkihYZKc+dK8+fbToRg8fzz0tmz0n33mdl6wSZfPrNDnyRNnGg3C/wH/aXgDqGh0uOPmzGv1QEAPijbhanhw4erUKFCFx0vUqSIhg0b5pZQADwoOlp68UUz7tFD+ucfm2kQDBYtkv73PzNL6s03baexx9UE/ZNPuO5wZadPm6V8Ev2lcO1cu/MtXiwlJtrNAgDABbJdmNq9e7fKli170fEbbrhBu3fvdksoAB42aJB0yy3SgQPSCy/YToNAduaM9NxzZty7t/m5C1Z16ki33WYaWs+caTsNfN369aY4VbRocF83cI/y5aXataXUVP79AQD4nGwXpooUKaIffvjhouPff/+9rrvuOreEAuBhUVHS5MlmPHmytHSp3TwIXOPGmf5SRYpIL79sO41dDkf6rKlJkySn024e+Db6S8HdXLOm3n+ff38AAD4l24WpDh06qFevXlq2bJlSU1OVmpqqpUuXqnfv3nr44Yc9kRGAJzRsaHYJk8wfyydP2s2DwHPokNkJUpKGDTN9loLd449LkZHS999LGzfaTgNf5ipMsYwP7tK+vRQRIf34o5SQYDsNAADnZLswNXToUNWuXVt33323cuTIoRw5cqhZs2a666676DEF+Jvhw81OPb/9Zpb3Ae40cKB09KhUrZrZrhxSwYLSgw+a8aRJdrPAd505Y3ZvlGh8DvcpWFB64AEzpgk6AMCHZLswFRERoTlz5mj79u2aOXOm5s2bp507d2rq1KmKiIjwREYAnpI3rzRhghm/9Za0YYPdPAgcCQnphZe33za7QsFwLef76CPp2DG7WeCbNmwwvcgKF5ZuvdV2GgQS13K+WbOklBS7WQAA+H/ZLky5lCtXTu3atdN9992nG264wZ2ZAHhTy5bSI49IaWlSly7mnXrgWjidUq9e5r5DB6l+fduJfEujRlK5ctLx49Ls2bbTwBe5lvHdcQf9peBe99xjGuofPmx2TAUAwAdkuzD14IMP6vXXX7/o+MiRI9WuXTu3hALgZXFxUqFCpu/EyJG208DfffKJtHKllCOHlMnvi6B3YRN04EL0l4KnhIVJjz5qxiznAwD4iGwXplasWKF77733ouMtWrTQihUr3BIKgJcVLiyNGWPGQ4dKW7fazQP/dfKk9MILZty/v1S6tN08vuqJJ6TwcLNk6/vvbaeBL0lJkVavNmP6S8ETXMv5/vtf6c8/7WYBAEBXUZg6fvx4pr2kwsPDlZSU5JZQACzo0MEs6ztzxizpS021nQj+aNQoafdu6frrpb59bafxXUWKpDchZtYUzrdpk3TihGlUfdttttMgEFWuLEVHmyLoRx/ZTgMAQPYLU5UqVdKcOXMuOj579mxVrFjRLaEAWOBwSOPHS3nySGvXSu++azsR/M2ePdKIEWb8xhtSzpx28/g613K+GTPMTDNAythfKuSqW4ECl+eaNcVyPgCADwjL7icMHDhQbdq00c6dO3XXXXdJkpYsWaJZs2Zp7ty5bg8IwItKlzY9gZ55RhowQLr/fonNDZBV/fqZncTuuEOi5+CVNWkilSkj/fGHNHeu1LGj7UTwBfSXgjc88ohZdr1xo/TzzxJvLgMALMr2W3GtWrXSZ599ph07duiZZ57R888/r3379mnp0qW6+eabPZERgDd16yY1bGiWknTrZnZWA65k1SqzJMThMP3K2EnsykJCzLJZieV8MM6eNRsHSPSXgmcVKSK5esYyawoAYNlVzRFv2bKlVq9erRMnTui3337TQw89pL59+6pKlSruzgfA20JCzB/JkZHSl1+aZUbA5aSmSr16mXFMjOldgqzp3Nlcc6tWsekApO++k44fl/LnlypVsp0Ggc61nG/GDPpKAgCsuurmBStWrNATTzyhEiVK6M0339Rdd92lb7/91p3ZANhSvrw0eLAZ9+kjHTpkNQ583Pvvmz+o8+WTXn3Vdhr/UrKkdN99ZsysKbiW8TVsKIWG2s2CwNeypWmyv3+/9PXXttMAAIJYtgpTBw8e1IgRI1SuXDm1a9dOefPmVXJysj777DONGDFCNWvW9FROAN7Wt6+Z+fLXX+mzYYALHT0qvfSSGQ8eLBUubDePP3I1Qf/gAyk52W4W2EV/KXhTZKTZkVdiOR8AwKosF6ZatWql8uXL64cfflBcXJz279+vd955x5PZANgUHi5NmWLetZ8zR/riC9uJ4ItefdXMqCtfXurRw3Ya/9S8uZk59eef0vz5ttPAltRUacUKM6a/FLzFtZxv/nzzRgMAABZkuTC1aNEidenSRa+88opatmypUKaYA4GvWjUzc0qSunfnRSsy+uUX0+hckuLipIgIq3H8VliY9OSTZsxyvuD1/fdSUpKUNy992uA9NWpIt94qnT4tffyx7TQAgCCV5cLUqlWrdOzYMVWvXl21a9fW2LFjdeTIEU9mA+ALBg+WypUzPShefNF2GviS2FgpJcX0KWne3HYa/9ali9nJcOlSaedO22lgg2sZX4MG9JeC9zgcUqdOZsxyPgCAJVkuTNWpU0eTJk3SgQMH1K1bN82ePVslSpRQWlqa4uPjdezYMU/mBGBLjhzpszgmTpSWL7caBz5i0SLpf/8zs31Gj7adxv/dcIPUrJkZT55sNwvsoL8UbHnsMbM76OrV0o4dttMAAIJQtnfly5Url5588kmtWrVKP/74o55//nmNGDFCRYoU0f333++JjABsa9RI6tbNjGNipFOn7OaBXSkp0nPPmXHv3tItt9jNEyhcTdCnTTP/jRE80tLoLwV7SpSQmjY14w8+sJsFABCUsl2YOl/58uU1cuRI7d27Vx999JG7MgHwRa+/bho079ghDRliOw1sGjdO2r7d7MA3cKDtNIGjVSupSBEpMVH6739tp4E3/fij9PffUu7cprcf4G2uJugffGAKpQAAeNE1FaZcQkND1bp1a33Brl1A4MqXTxo/3oxHjZI2bbKbB3YcPpxemBw2zPxcwD0iIqTOnc2YJujBxbWMr359szwW8LbWrU3j/V270n8eAQDwErcUpgAEiVatpIcfNu+mdunCcqNgNHCg2Z2xWrX0Igrcp2tXc//ll+YPRAQH+kvBthw5pIceMmOaoAMAvIzCFIDsGTNGKljQbG3+xhu208CbEhJMA3zJ/Bywc5j73Xyz1Lix5HRKU6faTgNvSEtLL0zRXwo2uZbzzZ0rHT9uNwsAIKhQmAKQPUWKmKKEJP3nP6bXEAKf02kanTudZtZcgwa2EwUuVxP0qVOl1FS7WeB5P/8s/fmnlDOnVKOG7TQIZvXrSzfdJJ04Ic2bZzsNACCIUJgCkH2PPiq1aCElJ5ulRzRKDXxz55pdw3LkMI3w4Tn/+peZlbh3r7R4se008LTz+0uFh9vNguDmcKTPmmI5HwDAiyhMAcg+h8M0Qs+dW1q1SpowwXYieNKpU1Lfvmbcr590/fV28wS6qCipY0czpgl64Fu+3NyzjA++4PHHzf2yZdLu3XazAACCBoUpAFfnhhuk4cPNuF8/XsAGslGjzP/f0qWlF16wnSY4uJbzLVgg7d9vNws8x+k0MxElClPwDWXKmCb8Tqf04Ye20wAAggSFKQBX75lnzPKT48el7t3NC1kElj170guQo0aZPjjwvIoVzbWVmipNm2Y7DTxl2zbp0CEzS65mTdtpAOP85Xz8XgcAeAGFKQBXLyREmjxZioiQFi6UPvrIdiK4W79+Zilfw4ZSu3a20wQX16ypKVPo4xaoXP2l6tWTIiPtZgFcHnzQvAnx66/S2rW20wAAggCFKQDXpkIFadAgM+7VSzp82G4euM+qVabY6HCYnRgdDtuJgku7dlK+fNLvv0tLlthOA0+gvxR8UZ48pjgl0QQdAOAVFKYAXLsXX5QqVzZbnvfpYzsN3CEtTerd24y7dpWqVrWbJxjlzGl2wJRogh6InM70GVMUpuBrOnUy93PmmFmzAAB4EIUpANcuPNwsNwoJkWbNkv73P9uJcK3ef1/avFnKm1d69VXbaYKXaznfZ58xGzHQ/PqrdPCgWcJXu7btNEBGd95pdmA9elT64gvbaQAAAY7CFAD3qFFDio0146eflpKS7ObB1UtKkgYMMOPBg6UiRezmCWbR0ebaSklhSU2gcc2WqlPHND8HfElIiPT442bMvz0AAA+jMAXAfV55RbrpJmnvXql/f9tpcLVefdXsFFa+vNSzp+00cM2amjSJHbICCf2l4Os6djT3X34pHThgNwsAIKBRmALgPjlzpvfCGT9eWrnSbh5k3y+/SHFxZvzWW2bHRdjVoYOUK5f5f7Nihe00cAf6S8Ef3HKLVLeu6Tk4Y4btNACAAEZhCoB7NW6cPsOja1fp9Gm7eZA9zz9vlo3de6/UooXtNJDMDlkdOpgxTdADw2+/Sfv2mf58derYTgNc2hNPmPvp05mxCQDwGApTANxv5EipeHEzw+M//7GdBlm1eLG0YIEUFiaNHm07Dc7nKvbOnSv99ZfdLLh2rtlStWubmaaAr2rf3jTo/+knsyEGAAAeQGEKgPvlzy+9+64Zjxwpffed1TjIgpQU6bnnzLhXL9NfCr6jZk2pcmUpOZklNYGA/lLwF/nzS61bmzFN0AEAHkJhCoBntG4ttWsnpaZKXbpIZ8/aToTLefddads2qXBhaeBA22lwIYeDJuiBhP5S8Ceu5XyzZklnztjNAgAISBSmAHjOO+9IBQqYGVNvvmk7DS7l8GFp8GAzHjbMvEMO3/Poo1JUlLRli7Rune00uFp//CHt3m2WzNarZzsNcGVNm0rFikl//iktXGg7DQAgAFGYAuA5RYuand0kacgQ03MKvmfgQOnoUalqValzZ9tpcCkFCphZiBJN0P2Za7ZUzZpmt0XA14WFSY89ZsYs5wMAeACFKQCe1bGj1KyZ2Z0vJsZsOw3fkZAgTZxoxmPGSKGhVuPgClzL+WbPlpKS7GbB1aG/FPyRaznfggVmli0AAG5EYQqAZzkc0nvvmZkBK1Yw08OXOJ1Snz7mvn17qWFD24lwJQ0aSBUqSCdPmn4v8D/0l4I/uv12qVo10y/yo49spwEABBgKUwA8r0wZ6bXXzPiFF6S9e63Gwf/79FPzR3JUlNk9Eb7vwibo8C979ki//25mJtavbzsNkD2uWVMs5wMAuBmFKQDe0bOnVKeOdOyY1L07u4rZduqU1LevGffrJ11/vd08yLqOHaWICGnzZnOD/3DNlqpeXcqTx24WILseeUQKDzf/7mzZYjsNACCAUJgC4B2hodLkyeZF7YIF0pw5thMFtzfflHbtkkqXll580XYaZEehQtK//mXGzJryL/SXgj8rVEhq2dKMmTUFAHAjClMAvOe226SXXzbjXr2kI0fs5glWe/dKw4eb8RtvSDlz2s2D7HMt55s5Uzpxwm4WZB39peDvXMv5Zsww/aYAAHADClMAvKt/f9NE9fBh6bnnbKcJTv36mebZDRpIDz1kOw2uRuPG0o03mqWxH39sOw2yYt8+accOKSTEXHuAP7r3Xum666SDB6X4eNtpAAABgsIUAO+KiJCmTDF/nM2YIS1aZDtRcFm92uzm5nBIY8aYe/ifkBCpa1czZjmff3DNlqpaVcqXz24W4GpFRJheU5L0/vtWowAAAodPFKbGjRunMmXKKCoqSrVr19b69esvee6kSZPUsGFDFShQQAUKFFCTJk0uez4AH1SrltS7txl362ZmfcDz0tLS/7t36WK2/ob/6tTJ9G5bu5ZGxP6AZXwIFK7lfJ9/Lv39t90sAICAYL0wNWfOHMXGxmrw4MHavHmzqlSponvuuUeHDh3K9Pzly5erQ4cOWrZsmdauXavSpUurWbNm2rdvn5eTA7gmQ4dKZcua7dNfesl2muAwfbq0aZOUN6/02mu20+BaFS8u3X+/GTNryvdRmEKgqFbN9IxMTmYpMQDALawXpkaPHq2YmBh17txZFStW1IQJE5QzZ05NnTo10/NnzpypZ555RtHR0apQoYImT56stLQ0LVmyxMvJAVyTXLmkiRPNeNw4s8QMnpOUJA0YYMaDB0tFitjNA/dwNUH/8EPp9Gm7WXBpBw9K27ebpbMNG9pOA1wbh8PM2JTYnQ8A4BZhNr/4mTNntGnTJg1w/bEkKSQkRE2aNNHatWuz9BwnT55USkqKChYsmOnjycnJSk5OPvdxUlKSJCklJUUpKSnXkN4+V35//z4QxBo1UminTgp5/305u3TR2Q0bpKgo26myxV+uw5BXXlFoYqKc5crpbLduko/nRRY1bqyw66+XY/dunZ0zR05X75cg4+vXoWPJEoVJclaurLO5c3P9wf899JDC+vWTY+1apfz0k3TLLT5/HQLBgOsQviQ7P4dWC1NHjhxRamqqihYtmuF40aJFtW3btiw9R79+/VSiRAk1adIk08eHDx+uV1555aLjX331lXIGyBbp8eyKAj8W3qSJ7vr8c0Vt367funTRtkcftR3pqvjydZhr/37d9fbbkqRv27fXoa+/tpwI7lS+Xj1V2L1b/7zxhlbnz287jlW+eh1WnjFDZSX9Vrq0tixcaDsO4BZ1oqNVdPNm/fbKKxl+d/vqdQgEE65D+IKTJ09m+VyrhalrNWLECM2ePVvLly9X1CVmWQwYMECxsbHnPk5KSjrXlypv3rzeiuoRKSkpio+PV9OmTRUeHm47DnDVHOHhUvv2umX+fN344otSlSq2I2WZP1yHof/6l0LOnlVa8+aqMWiQ7Thwt0qV5Pz4YxX66Sfde/PN0i232E7kdb5+HYb9/8zwGzp21PX33ms5DeAejuPHpcce0y3ffqsbP/xQKampPn0dAsHA138fIri4VqtlhdXCVKFChRQaGqrExMQMxxMTE1WsWLHLfu6oUaM0YsQIff3116pcufIlz4uMjFRkZORFx8PDwwPmYg2k7wVB6qGHpDlz5Jg3T+FPPy19+60U5l91c5+9Dr/8Uvrf/6SwMIW89ZZCfDEjrs2NN0rNm0sLFyp8+nRp5Ejbiazxyevw0CFp61ZJUthdd0m+lg+4Wg8+KPXoIceePQpftUq64w5JPnodAkGG6xC+IDs/g1abn0dERKh69eoZGpe7GpnXrVv3kp83cuRIDR06VIsXL1aNGjW8ERWAp40dK+XPb3aNi4uznSYwpKRIzz1nxs8+K1WoYDcPPMfVBP3996UzZ6xGwQVWrDD3lSpJ111nNwvgTlFRUvv2ZkwTdADANbC+K19sbKwmTZqk6dOna+vWrerevbtOnDihzp07S5I6duyYoTn666+/roEDB2rq1KkqU6aMDh48qIMHD+r48eO2vgUA7lC8uPTmm2Y8cKC0Y4fdPIHg3XfNTI3ChSWW8AW2li2lYsWkw4elzz+3nQbn++Ybc9+okd0cgCc88YS5//RT6dgxu1kAAH7LemGqffv2GjVqlAYNGqTo6GglJCRo8eLF5xqi7969WwcOHDh3/vjx43XmzBm1bdtWxYsXP3cbNWqUrW8BgLt07izdfbfZ9j4mRnI6bSfyX4cPS4MHm/Frr5nZaAhc4eHSk0+a8aRJdrMgIwpTCGR160rlykknT8oxf77tNAAAP2W9MCVJPXv21K5du5ScnKx169apdu3a5x5bvny53n///XMf//HHH3I6nRfdhgwZ4v3gANzL4ZAmTpRy5pSWL5cmT7adyH8NGiQdPSpFR6cXLBDYunQx9/Hx0u+/280C488/pR9/NOP/778DBBSH49ysqZB33lHJFSvk+OYbKTXVcjAAgD/xicIUAJxz443Sq6+acd++0r59dvP4o++/NwU+SRozRgoNtZsH3nHjjVKTJmY8ZYrdLDBc/aUqVpSKFLGbBfCUQoUkSSHff68ao0crrGlTqUwZad48u7kAAH6DwhQA39Orl1SrlpSUJPXowZK+7HA6pT59pLQ0s9shszSCi6sJ+rRp0tmzdrOAZXwIfPPmSd27X3x83z6pbVuKUwCALPGv/dgBBIfQUDPjo1o108h57lypXTvbqfzDvHlmGWRUlPTGG7bTwNseeMDMXti/X1q4ULr/ftuJghuFKQSy1FSpd+/M3zxyHYuJMTvE5solRUaa302u2/kfu8YREWZ5ILwvNVVauVI6cMBsSNOwITOuAXgNhSkAvun226UBA6T//Efq2VO66y62Wr+SU6ek55834379pOuvt5sH3hcZafq9vPmmaYJOYcqev/82y2olClMITCtXSnv3Xv6cv/6SHn44e897qaLV5Qpa1/LYhedFREghQbaoZN48U2Q8//9nqVKmHUCbNvZyAQgaFKYA+K6XXjKzpX7+2RRcztsIAZl4801p1y7zYvLFF22ngS1du5qfhYULzR8ZpUrZThScVq40s0bKl5eKFbOdBnC/83bNvqzy5aW8ec2Ou8nJ5v78cXJyxvNdjx896v7MWRUR4Z2i2KUei4z03mylefPMsssLZ765lmPOnUtxCoDHUZgC4LsiI83OfPXrS9OnSx06SPfcYzuVb9q7Vxo+3IzfeMPsbIjgVKGC6S22YoU0darZoRHexzI+BLrixbN23oQJ0p13XvrxtDTpzJnMi1aZfeyJx06fzpjpzBlzsyk83D0zwC73WFiY6RF2qeWYDofpW/nAAyzrA+BRFKYA+La6dU0z9DFjpG7dpC1bpNy5bafyPf37SydPmiJe+/a208C2mBhTmJoyRfr3v/mDwgYKUwh0DRuaGZn79mVe2HA4zOMNG17+eUJC0gsl+fJ5JuuVOJ2mF5anCmFZOe/UqYz/HVNSzO34cTv/TVz/XfbsMTNAL1dcBIBrRGEKgO979VXps8/MMrV//9sUqZBuzRpp5kzzR8Dbb9M4FtKDD0rPPivt3i3Fx0vNm9tOFFyOHpW++86MKUwhUIWGmt/Hbdua3zvnF1Vcv4fi4vyjMO5wmOV7ERFm2aENTqfZTdWbM8eOHJEOHbpytqwu2wSAq0RhCoDvy51bmjjRLON75x3TSLVuXdupfENammlYKkldupidDIEcOaTHHzfXy6RJFKa8bdUqc23efLNUsqTtNIDntGljehBl1jg7Lo7eRNnhcJjle+Hh3psZvny51Ljxlc/L6rJNALhKQbblBAC/1ayZ2W3M6TQFmAubpQar6dOljRvNO7yvvmo7DXxJTIy5/+ILKTHRbpZgwzI+BJM2baQ//tDZ+HhtjI3V2fh46fffKUr5A9dyzCvNtF6/3hTbAcBDKEwB8B+jR0tFikhbt0rDhtlOY19SkjRggBkPGiQVLWo3D3xLpUpS7dpmaQg7WnqXqzBFTxYEi9BQORs10r477pCzUSP/WL6H9OWY0sXFqfM/7tdPatFCOnjQe9kABBUKUwD8R8GC0tixZjxsmPTjj3bz2Pbaa2YmTLlypp8QcCHXrKnJk3m321uOHZM2bTJjZkwB8HWu5ZgXLjsuVcocf+89szz8q6+kypWlhQvt5AQQ0ChMAfAvbdtKrVubWSBdukipqbYT2fHrr9Jbb5nxW2+Zhq3Ahdq3N71KduwwvUTgeatXm3+XypaVSpe2nQYAruz/l2Nq2TJp1ixz//vvZiONp54yLQMqV5YOH5ZatpSee46WCgDcisIUAP/icEjjxpktpTdsCN4d+vr2NdtIN28u3Xuv7TTwVblzS48+asaTJtnNEizoLwXAH4WGmuXHHTqY+/OXY1asKK1bJ/XqZT6Oi5Pq1JG2bbMQFEAgojAFwP+UKCGNGmXGL78s/fab3Tze9tVXpqF1WJiZLXWlpqUIbq7lfPPmma3B4Vn0lwIQiKKizJuB//2vVKiQlJAgVa9uloo7nbbTAfBzFKYA+KcuXcwWx6dOmT+8g+VFUUqK1KePGT/7rFShgtU48APVq0tVq0pnzkgffmg7TWA7ccLM5JSYMQUgMN13n/TDD1KTJtLJk+Y12EMPSX//bTsZAD9GYQqAf3I4pIkTzTt4S5dK06bZTuQd48ebXQkLFTI78QFZ4Zo1NWlS8BRxbVizxvS/u/56qUwZ22kAwDOKF5e+/FJ6/XUze3vuXCk6Wlq1ynYyAH6KwhQA/3XzzdLQoWYcGysdOGA3j6cdOSINHmzGr70m5c9vNQ78yCOPSDlzmqLmmjW20wQu+ksBCBYhIdKLL5rfKTfdJO3ebf7te+UVU6AHgGygMAXAv/XpY5YqHT0q9expO41nDRok/fOPVKWKWcoIZFW+fGaphUQTdE+ivxSAYFOzpvTdd1LHjlJamjRkiGm1sHu37WQA/AiFKQD+LSxMmjLF3M+bJ336qe1EnvHDD9J775nx229n3C0HyArXcr6PPzYFTrjXyZNm1yqJGVMAgkuePNL06dLMmWa8apV5E23uXNvJAPgJClMA/F+VKlK/fmbco0fgNeB0Os3MsLQ0M+vljjtsJ4I/qlvXbPl96pT54wHu9e23ZnOCkiWlG2+0nQYAvO+RR8xufbVqmTdA2rUzb4qcOGE7GQAfR2EKQGB4+WWzQ11iovT887bTuNe8edKyZabR+8iRttPAXzkc0lNPmTFN0N3v/P5SDofdLABgy403mhlTAwaYfwsnT5Zq1JC+/952MgA+jMIUgMAQFWVe/DgcZoe+r7+2ncg9Tp2S+vY14xdflG64wW4e+LfHH5ciI80fCBs32k4TWOgvBQBGeLg0bJh5LVaihLRtm5lFNWYMb4oAyBSFKQCBo359s5RPCpyp46NHS3/8IZUqZQpTwLUoWFB68EEzpgm6+5w+bZbySfSXAgCXu+4yb4Tcf7905oxpS3DffdKhQ7aTAfAxFKYABJZhw6TSpU0xZ+BA22muzb595vuRzBK+XLns5kFgcDVB/+gj6fhxu1kCxbp1UnKyVKyYVK6c7TQA4DsKFZI++0waN87M2F240PQGjY+3nQyAD6EwBSCw5MmTvntdXFz6Lln+qH9/s9NX/frSww/bToNA0aiRKZ4cPy7Nnm07TWCgvxQAXJrDIT3zjLRhg3TbbdLBg1KzZmYm+JkzttMB8AEUpgAEnhYtpMceM30MunTxzxc9a9dKM2aYF3NjxvDHLtzH4ZC6djVjlvO5B/2lAODKKlUyxanu3c3Hb7xh3nz79Ve7uQBYR2EKQGB66y2pcGHpp5+k4cNtp8metDSpd28zfvJJqXp1u3kQeJ54QgoLk9avZ6eka5WcLK1ZY8b0lwKAy8uRQ3r3XWn+fNP3cONGqWpVafp0GqMDQYzCFIDAVKiQ9PbbZvzaa6ZA5S8++MC8o5gnj8kOuFvRolLr1mbMrKlrs2GDaX5epIhUoYLtNADgH1q3Nm+M3Hmn2aymUyfp0Uelo0ctBwNgA4UpAIGrfXupVSspJcUs6UtNtZ3oypKSTG8pSRo0yBQQAE9wNUGfMcP0MsPVcS3ju+MOltwCQHaUKiV9/bV5Ey401GzKUbVq+i6nAIIGhSkAgcvhMNPF8+QxTdDHjrWd6MqGDZMSE01z6l69bKdBIGvSRCpTxrw7PXeu7TT+i/5SAHD1QkOll16SVq0yv5N+/11q0MC8HvKHNxQBuAWFKQCBrVQp01xTMi98fv/dbp7L2bHD9MaSpNGjpYgIu3kQ2EJCzExCieV8VyslRVq92ozpLwUAV69OHSkhQerQwRSk/v1v8wbKvn22kwHwAgpTAAJfTIxZZnPypNStm+821+zb1+wg2Ly51LKl7TQIBp07mwLVqlXS1q220/ifjRvNvyvXXSdVrGg7DQD4t3z5pJkzpfffl3LlkpYvlypXlj7/3HYyAB5GYQpA4AsJMTNCoqKk+Hiz84uviY83L7zCwsxsKXrVwBtKlkwvgk6ebDeLPzq/v1QIL6kA4Jo5HGbn2M2bza7Ef/1lGqU/84x06pTtdAA8hFdRAILDLbdIQ4aYcWys6ePkK1JSpD59zLhnT+nWW63GQZBxNUGfPl1KTrabxd8sX27u6S8FAO51yy3SmjVmNrkkjR8v1awpbdliNxcAj6AwBSB4PP+8VK2a9Pff0rPP2k6TbsIE6eefpUKFzE58gDe1aCGVKCH9+ac0f77tNP7j7Fn6SwGAJ0VEmD6hX35pdin+6SepRg1p3DjfbcsA4KpQmAIQPMLCpClTzA4wn3wiffaZ7UTSkSPpxahXX5UKFLCbB8EnLIwm6Fdj82bp+HFzzVaqZDsNAASuZs2kH36Q7r3XzOzt2dMs7ztyxHYyAG5CYQpAcImOll580YyfeUb65x+baaTBg02GKlWkrl3tZkHw6tLF9PVYulTaudN2Gv9AfykA8J4iRaQFC6S4ODOT6osvzGunZctsJwPgBrySAhB8Bg0yvQsOHJBeeMFejh9/NMv4JGnMGDOTC7DhhhvMO9ISTdCzytVfimV8AOAdDofUu7e0bp1Uvry0f790993SSy+Zfp0A/BaFKQDBJyoq/Y/vyZPNLBFvczrNi6u0NKldO/64hX2uJujTpvEC/0pSU6VVq8yYaxcAvCs6Wtq0ycw0dzql4cOlhg2l336znQzAVaIwBSA4NWwode9uxjEx0smT3v368+eb6edRUdLIkd792kBmWrUySyUSE81yCVxaQoKUlCTly2eWkgAAvCtXLtMX8ZNPpPz5zSyq6Ghp5kzbyQBcBQpTAILXiBFSqVLmHbbBg733dU+fNjsESmYpYZky3vvawKVEREidOpkxTdAvz9VfqmFDluACgE1t20rffy81aCAdOyY99pj0xBNmDMBvUJgCELzy5k3v8TR6tLRhg3e+7ujR0h9/SCVLSv36eedrAlnhasC/eLG0a5fdLL6M/lIA4Duuv97MQh8yxGxG8cEHUrVq3ntdB+CaUZgCENxatpQeecT0eurSRTpzxrNfb98+adgwMx450kxFB3xFuXJS48amZ8fUqbbT+KbUVGnlSjOmMAUAviEszMx+/+YbU6jasUOqV8+81kpLs50OwBVQmAKAuDjpuuvMLnme7vc0YIB04oRUv77UoYNnvxZwNVxN0KdONUUYZPTjj9I//0h58khVq9pOAwA4X4MGpg9g27bS2bNmZvo995idmAH4LApTAFC4sPT222Y8dKi0datnvs6330offmi2Ox4zxtwDvuZf/5IKFpT27jVL+pCRq79UgwbmHXoAgG8pUED6+GPTLzFnTunrr6XKldnYA/BhFKYAQDKzl+691yzl69rV/dO+09KkXr3MuHNnqXp19z4/4C5RUVLHjmZME/SL0V8KAHyfw2Fez23aZHbrO3LE7D7bq5fZhAaAT6EwBQCSeQEzYYKUO7e0Zo307rvuff4PPzRNOPPkkV57zb3PDbibaznfggUsfzhfWpq0YoUZU5gCAN9XoYKZsd6nj/n4nXek2rU9NzsewFWhMAUALqVLS6+/bsb9+7tvV7Jjx8zzSdLAgVKxYu55XsBTKlY0TWNTU6Vp02yn8R0//ST99ZfZtIBZjwDgHyIjpbfekv73P9O+4YcfzL/hEyeazT4AWEdhCgDO9/TTpnfMiRNm7I4XLMOGSQcPSjffnL6cD/B1rllTkyezo5GLq79U/fpSeLjdLACA7Ln3XlOUatpUOnVK6tbNNEn/6y/byYCgR2EKAM4XEmL+EI+MNI2fZ8y4tufbuVMaPdqM33rLPC/gD9q1k/LmlX7/XVqyxHYa30B/KQDwb8WKmdd3b7xh3mCYN0+qUiV9mTYAKyhMAcCFypeXBg824z59pEOHrv65nn/eNFS/5x6pZUu3xAO8Ilcu6bHHzJgm6Gb2JP2lAMD/hYRIfftKa9dK5cqZXWgbN5YGDZLOnrWdDghKFKYAIDN9+5pdXP766+qX38XHS59/LoWGmtlSDodbIwIe51rO99ln0uHDVqNYt3Wr+W+QI4dUs6btNACAa1W9urR5s9Spk1myPnSoeePBXT1GAWQZhSkAyEx4uDRliikqzZkjffFF9j7/7Nn0HWB69pRuvdXtEQGPi46WatSQUlKk6dNtp7HL1V+qXj0pIsJuFgCAe+TObTb5+Ogjs3x9zRqztO/jj20nA4IKhSkAuJRq1cxSPEnq3l06ejTrnzthgvTzz9J116UvCwT80flN0IN59yL6SwFA4Hr4YSkhQapTx7zea99e6trVbIYDwOMoTAHA5QwZYnbT279f6tcva5/z55+mT4EkvfqqVKCAx+IBHtehg+k3tX27tHKl7TR2OJ3pM6YoTAFAYCpb1vQS/Pe/TfuFKVPMcr/vvrOdDAh4FKYA4HJy5Ehv/Pzee+l/nF7O4MHS339LlSunzzYB/FWePOadZCl4m6D/8ouUmGh21axVy3YaAICnhIebNxWXLpVKljRvytSpY3qFpqXZTgcELApTAHAld94pPfWUGXftKp06delzf/xRGj/ejMeMMT2qAH/nKrB+8onZECDYuJbx1a0rRUVZjQIA8II775S+/15q3drsrhwba3ZXTky0nQwISBSmACArRo6USpSQduwwy/sy43RKvXubd9TatjUvaoBAUKuWVKmSlJwszZhhO433sYwPAILPdddJ8+aZNxyjoqTFi01j9C+/tJ0MCDgUpgAgK/LlS58J9eab0qZNF5/z2WfSsmVmuc8bb3g1HuBRDkf6rMFJk4KrCTr9pQAgeDkc0tNPSxs2SLffbmZMNW9uNsdJTradDggYFKYAIKvuv9/s0pKaKnXpIqWkpD92+nT6Dn4vvCCVKWMlIuAxjz5q3jHeskVat852Gu/ZudNsfhARYfqMAACCz+23S+vXSz16mI9Hj5bq1TM9CAFcMwpTAJAdb78tFSxo+g6MHCnHN9+o5IoVCunTR/r9d9Mos39/2ykB9ytQQGrXzoyDqQm6q79U7dpmMwQAQHDKkUMaO1b6/HOzzG/zZqlaNWnatOCaSQx4AIUpAMiOIkWkuDgzfvllhTVtqhqjRyt06lRzrF07KVcua/EAj3I1QZ89W0pKspvFW1jGBwA43/33mzcoGzeWTpyQnnxS6tBB+ucf28kAv0VhCgCyK2fOSz82ZoxplAkEogYNpAoVpJMnpY8+sp3G8+gvBQDITMmSUny8NGyY2YF5zhwpOlpas8Z2MsAvUZgCgOxITZX69Ln8OX36mPOAQONwSF27mnEwLOf74w9pzx4pPFyqW9d2GgCALwkNlQYMkFavlsqWlXbtku64Qxo6lNeBQDZRmAKA7Fi5Utq799KPO53mD9mVK72XCfCmjh1NoWbTJtNfI5C5+kvVrMkSXQBA5mrXlhISzCYhqanSoEHS3Xeb14MAsoTCFABkx4ED7j0P8DeFC0tt2phxoM+aYhkfACAr8uaVZsyQPvhAyp3b/P6oUkWaP992MsAvUJgCgOwoXty95wH+yNUEfeZM0/g1UFGYAgBkx+OPS999Z2ba/v23eSPn6adNb0YAl0RhCgCyo2FDqVQp02snMw6HVLq0OQ8IVI0bSzfeKB07Jn38se00nrFrl+kxFRoq1a9vOw0AwF/cfLO0apX04ovm4/feM4WqH36wmwvwYRSmACA7QkPNznvSxcUp18dxceY8IFCFhAR+E3TXbKkaNcyyDAAAsioiQnr9dbNzX7Fi0s8/S7VqSe+8Y/qRAsiAwhQAZFebNtLcuWar4POVKmWOu/rvAIGsUydTgF27VvrpJ9tp3I9lfACAa9WkiZkp1bKllJws9eol3X+/dPiw7WSAT7FemBo3bpzKlCmjqKgo1a5dW+vXr7/kuT/99JMefPBBlSlTRg6HQ3Fxcd4LCgDna9NG+uMPnY2P18bYWJ2Nj5d+/52iFIJH8eJSq1ZmHIizpihMAQDcoXBh6b//ld5+W4qMlBYsMI3RlyyxnQzwGVYLU3PmzFFsbKwGDx6szZs3q0qVKrrnnnt06NChTM8/efKkbrzxRo0YMULFihXzcloAuEBoqJyNGmnfHXfI2agRy/cQfFxN0D/4QDp92m4Wd9q7V9q50yxZbNDAdhoAgL9zOKRnn5XWr5duvdXs3ty0qdS/v5SSYjsdYJ3VwtTo0aMVExOjzp07q2LFipowYYJy5sypqVOnZnp+zZo19cYbb+jhhx9WZGSkl9MCAIAM7rnHNPv/+2/p009tp3Ef12ypatXMFuAAALhD5crSxo3SU0+ZXlOvv2422Ni503YywKowW1/4zJkz2rRpkwYMGHDuWEhIiJo0aaK1a9e67eskJycrOTn53MdJSUmSpJSUFKX4eXXald/fvw/An3EdItiFdOqk0KFDlTZxolIfeshKBndfh6HLlilEUmrDhkrj2gayhN+HQBaFh0tjx8px990KffppOTZskDM6Wqlvvy3nY49d01NzHcKXZOfn0Fph6siRI0pNTVXRokUzHC9atKi2bdvmtq8zfPhwvfLKKxcd/+qrr5QzZ063fR2b4uPjbUcAgh7XIYJV1PXXq1lIiEJWrNDSSZN04sJNAbzIXdfh3YsWKbekDTlyKHHhQrc8JxAs+H0IZFFkpKJGjlT1uDgV+uknhT35pPZMn64funXT2Wv8O5XrEL7g5MmTWT7XWmHKWwYMGKDY2NhzHyclJal06dJq1qyZ8vr59PyUlBTFx8eradOmCg8Ptx0HCEpch4DknDdPjkWL1HjnTqW5+k55kVuvwwMHFL5/v5wOh6r36SPlz++OiEDA4/chcJUee0ypI0Yo5NVXVfqbb1Rqzx6lfvCBnLVqZfupuA7hS1yr1bLCWmGqUKFCCg0NVWJiYobjiYmJbm1sHhkZmWk/qvDw8IC5WAPpewH8FdchgtpTT0mLFin0ww8VOny4FBFhJYZbrsM1ayRJjuhohRcu7IZUQHDh9yGQTeHh0pAhUrNm0iOPyPHbbwq7805p6FDpxRfNRhzZfkquQ9iXnZ9Ba83PIyIiVL16dS05b5vMtLQ0LVmyRHXr1rUVCwAAZFfLllKxYtLhw9IXX9hOc21cjc8bNbKbAwAQXOrVkxISpPbtpbNnpQEDzM59+/fbTgZ4nNVd+WJjYzVp0iRNnz5dW7duVffu3XXixAl17txZktSxY8cMzdHPnDmjhIQEJSQk6MyZM9q3b58SEhK0Y8cOW98CAAAID5f+/3e3Jk2ym+VaUZgCANiSP7/00UfSlClSzpzS0qVmJ7///td2MsCjrBam2rdvr1GjRmnQoEGKjo5WQkKCFi9efK4h+u7du3XgwIFz5+/fv19Vq1ZV1apVdeDAAY0aNUpVq1ZV165dbX0LAABAkrp0MfdffSX9/rvdLFcrMVHaulVyOKQ77rCdBgAQjBwO6cknpc2bpapVpT//lO6/X+rZUzp1ynY6wCOsFqYkqWfPntq1a5eSk5O1bt061a5d+9xjy5cv1/vvv3/u4zJlysjpdF50W758ufeDAwCAdDfdJN19txlPmWI3y9VascLcV6okFSxoNwsAILiVLy+tXSu5NvIaN06qVUv66Se7uQAPsF6YAgAAAeKpp8z9tGmmP4a/YRkfAMCXREZKb74pLVokFSkibdki1aghTZggOZ220wFuQ2EKAAC4xwMPSIUKmUatCxfaTpN9rsLUnXdajQEAQAbNm0s//GDuT5+WuneX2rSR/vrLdjLALShMAQAA94iMlJ54woz9rQn6kSPmnWiJ/lIAAN9TtKj0v/9Jo0ebTUc++0yqUiX9TZXUVDm++UYlV6yQ45tvpNRUq3GB7KAwBQAA3Me1IcnChdLevXazZIerv9Rtt5lZXwAA+JqQEOm556Rvv5VuucX8nm3cWGrXTipTRmFNm6rG6NEKa9pUKlNGmjfPdmIgSyhMAQAA96lQQWrYUEpLM72m/AX9pQAA/qJaNWnTJrN7n9MpzZ178ZtB+/ZJbdtSnIJfoDAFAADcKybG3E+Z4j9LCegvBQDwJ7lzSxMnXnoXWVdz9D59/Od3MYIWhSkAAOBebdtK+fNLu3ZJ8fG201zZX3+ZprIS/aUAAP5j5crLN0B3OqU9e6T27aXx46UlS8zMKnb0g48Jsx0AAAAEmBw5pMcfl955xzRBb97cdqLLW7nSvEivUME0lwUAwB8cOJC18z791NxccuY0ParKl7/4Pm9ez2QFLoPCFAAAcL+YGFOY+uILKTHRtws+9JcCAPij4sWzdt5DD0knT0rbt0u//WbGCQnmdqFixTIvWt14o9kNEPAAClMAAMD9KlWSateW1q2T3n9f6tfPdqJLo78UAMAfNWwolSplGp1ntjzP4TCPz5olhYaaYykppjj1yy+mUOW6377dvJF08KC5uXardQkLM8WpzIpWxYqZrwVcJQpTAADAM2JiTGFq8mTpxRd980XrP/9I331nxsyYAgD4k9BQacwY09vR4chYnHL9zo2LSy9KSWbWU/ny5taqVcbnO3rUFKouLFr98ouZZeV6bMGCjJ+XN68pUF1YtCpXzjRpB66AwhQAAPCM9u3NbkA7dkjLl0uNG9tOdLFVq8wL+XLlsr4kAgAAX9GmjTR3rtS7t2ls7lKqlClKtWmT9efKl0+qWdPczpeWJu3ff3Gxavt26Y8/pKQkaeNGc7tQyZKZ97K64QYzCwsQhSkAAOApuXNLjzxitrOeNMk3C1P0lwIA+Ls2baQHHtDZZcuUsGiRolu0UFjjxhlnSl2LkBBT6CpVSrr77oyPJSdLO3devCzwl1+kI0fMMsN9+6SlSzN+XkSEdNNNmRetChXyzVnW8BgKUwAAwHNiYkxh6tNPzQvUQoVsJ8qI/lIAgEAQGipno0bad+KEqjRq5L6i1JVERkoVK5rbhf76K/NeVr/+agpaW7ea24Xy509fbnh+0apcObPzLwIOhSkAAOA51atL0dFm558PP5See852onRJSdKmTWbMjCkAANyrYEGpTh1zO19amrR7d+ZFq927Tf/HdevM7XwOh1S6dOZFq+uvNzO74JcoTAEAAM9xOKSnnpKeecYs5+vTx3em569ebV4c33ijWZ4AAAA8LyREKlPG3Jo1y/jYqVNmRlVmRat//jGFq927pfj4jJ8XFSXdfHPmRauCBb30jeFqUZgCAACe9cgjUt++Zrr+mjVS/fq2Exn0lwIAwLfkyCFVrmxu53M6TUuAzBqw79ghnT4tbdlibhcqVCi9UHV+0eqmm8xSRFhHYQoAAHhWvnzSQw9J779vZk35WmGK/lIAAPg2h0MqXNjcGjTI+NjZs9KuXZkXrfbtMwWtI0fMm2Pnc83cyqxoVbKk78zwDgIUpgAAgOfFxJjC1Mcfm+2r8+e3m+f4cWnDBjNmxhQAAP4rLMzMfrrpJuneezM+dvy4WRqY2a6Bx45Jv/1mbosXZ/y8nDlNkerCotUtt5g33OBWFKYAAIDn1a1rduz5+Wdp1izTc8qmNWuk1FTphhvMDQAABJ7cuaWqVc3tfE6ndPDgxcWq7dtNoerkSbNxS0LCxc9ZtGjG2VWu8Y03SuHhV581NVVauVI6cEAqXlxq2NB7uytaRmEKAAB4nsNhZk0995w0caLUvbvdKfL0lwIAIHg5HKb4U7z4xa8FUlJMcSqzolViYvptxYqMnxcaaopTmRWtihW7/OueefOk3r2lvXvTj5UqJY0ZI7Vp477v20dRmAIAAN7x+ONSv37S999LGzdKNWvay0J/KQAAkJnw8PSiUqtWGR87etQUqS4sWv3yi5ll9euv5nahPHky7hToev5y5aSvvpLatjWzuM63b585PnduwBenKEwBAADvuO468wJr1izTBN1WYerkSWn9ejNmxhQAAMiqfPnM65cLX8OkpUn792fey+qPP0w/q02bzO1CISEXF6Ukc8zhkPr0kR54IKCX9VGYAgAA3hMTYwpTH30kjR5tej9429q1Zpp+qVJS2bLe//oAACCwhISY1xWlSkl3353xseRkaefOzItWR46YotalOJ3Snj2m91QAz/KmMAUAALynUSMzbf3XX6XZs6WuXb2f4fxlfGwFDQAAPCky0mwAU7HixY9NmiQ99dSVn+PAAffn8iEhtgMAAIAg4nCkF6MmTbKTYflyc88yPgAAYFO5clk7r3hxz+awjMIUAADwrieekMLCTJ+nH37w7tc+dUpat86MKUwBAACbGjY0y/8uNYPb4ZBKlzbnBTAKUwAAwLuKFjVNPCXvz5pat046c8a883jzzd792gAAAOcLDZXGjDHjC4tTro/j4gK68blEYQoAANgQE2PuZ8wwu+R5C/2lAACAL2nTRpo7VypZMuPxUqXM8TZt7OTyIpqfAwAA72vaVLrhBmnXLvOiq2NH73xd+ksBAABf06aNmU2+cqVpdF68uFm+F+AzpVyYMQUAALwvJMT7TdCTk6VvvzVjClMAAMCXhIaaGd0dOpj7IClKSRSmAACALZ07mwLVqlXS1q2e/3rr10unT5seV+XLe/7rAQAA4IooTAEAADtKlpRatjTjyZM9//Vc/aUaNaK/FAAAgI+gMAUAAOxxNUGfPt0stfMk+ksBAAD4HApTAADAnhYtpBIlpD//lD77zHNf58wZac0aM6YwBQAA4DMoTAEAAHvCwqQnnzRjTzZB37hROnVKKlRIqljRc18HAAAA2UJhCgAA2NWli+n5tGSJtHOnZ74G/aUAAAB8EoUpAABgV5kyUtOmZuypJuj0lwIAAPBJFKYAAIB9Tz1l7qdNk1JS3PvcKSnS6tVmTGEKAADAp1CYAgAA9rVqJRUpIiUmSgsWuPe5N2+WTpyQChaUbr/dvc8NAACAa0JhCgAA2BcRIXXqZMbuboLu6i91xx1SCC99AAAAfAmvzgAAgG/o2tXcL14s7d7tvuelvxQAAIDPojAFAAB8Q7ly0p13Sk6nNHWqe57z7Flp1SozpjAFAADgcyhMAQAA3xETY+6nTJFSU6/9+RISpGPHpHz5pMqVr/35AAAA4FYUpgAAgO9o08Y0Kd+71yzpu1bn95cKDb325wMAAIBbUZgCAAC+IypK6tjRjN3RBJ3+UgAAAD6NwhQAAPAtruV8CxZIBw5c/fOkpkorV5oxhSkAAACfRGEKAAD4looVpXr1TGFp2rSrf54ffpCOHpXy5pWio90WDwAAAO5DYQoAAPge16ypyZOltLSrew7XMr4GDaSwMLfEAgAAgHtRmAIAAL6nXTsz0+n336WlS6/uOVyNz1nGBwAA4LMoTAEAAN+TK5f06KNmfDVN0NPS6C8FAADgByhMAQAA3+Razjd/vnT4cPY+d8sW6a+/pNy5pWrV3J8NAAAAbkFhCgAA+KaqVaXq1aWUFGn69Ox9rqu/VP36Uni426MBAADAPShMAQAA3/XUU+Z+8mTJ6cz659FfCgAAwC9QmAIAAL6rQwfTb2r79vSeUVfidEorVpgxhSkAAACfRmEKAAD4rjx5pIcfNuOsNkH/+WfpyBEpZ06pRg3PZQMAAMA1ozAFAAB8m6sJ+ty50t9/X/l8V3+pevWkiAiPxQIAAMC1ozAFAAB8W61aUqVK0unT0owZVz6f/lIAAAB+g8IUAADwbQ5H+qypSZMu3wTd6aQwBQAA4EcoTAEAAN/32GNSVJT044/SunWXPm/7dunQIXNurVreywcAAICrQmEKAAD4vgIFpLZtzfhyTdBd/aXq1pUiIz0eCwAAANeGwhQAAPAPTz1l7mfPlpKSMj+HZXwAAAB+hcIUAADwDw0aSBUqSCdPSh99dPHj9JcCAADwOxSmAACAf3A4pK5dzTiz5Xw7dkgHDpglfHXqeDcbAAAArgqFKQAA4D86dpTCw6VNm6TvvsvwkGPFCjOoXds0PwcAAIDPozAFAAD8R+HC0r/+ZcYXzJoKcRWmWMYHAADgNyhMAQAA/xITY+5nzJBOnDBjp1OOlSvNmMIUAACA36AwBQAA/Mtdd0lly0rHjkkffyxJypmYKMfevWaZX926lgMCAAAgqyhMAQAA/xISkj5r6v+X8xXassV8XKuWlDOnpWAAAADILgpTAADA/3TqJIWGSmvXSj/9pOt++skcZxkfAACAX/GJwtS4ceNUpkwZRUVFqXbt2lq/fv1lz//kk09UoUIFRUVFqVKlSlq4cKGXkgIAAJ9QvLjUqpUkKeQ//1HRjRvN8QYNLIYCAABAdlkvTM2ZM0exsbEaPHiwNm/erCpVquiee+7RoUOHMj1/zZo16tChg7p06aLvvvtOrVu3VuvWrbXFNYUfAAAEh4oVJUmh8+cr8tgxcywmRpo3z2IoAAAAZIf1wtTo0aMVExOjzp07q2LFipowYYJy5sypqVOnZnr+mDFj1Lx5c73wwgu69dZbNXToUFWrVk1jx471cnIAAGDNvHnS8OEXH9+/X2rbluIUAACAn7BamDpz5ow2bdqkJk2anDsWEhKiJk2aaO3atZl+ztq1azOcL0n33HPPJc8HAAABJjVV6t1bcjovfsx1rE8fcx4AAAB8WpjNL37kyBGlpqaqaNGiGY4XLVpU27Zty/RzDh48mOn5Bw8ezPT85ORkJScnn/s4KSlJkpSSkqKUlJRriW+dK7+/fx+AP+M6BLzP8c03Ctu799InOJ3Snj06u2yZnDRDB7yC34eAfVyH8CXZ+Tm0WpjyhuHDh+uVV1656PhXX32lnAGynXR8fLztCEDQ4zoEvKfkihWqkYXzEhYt0r4TJzyeB0A6fh8C9nEdwhecPHkyy+daLUwVKlRIoaGhSkxMzHA8MTFRxYoVy/RzihUrlq3zBwwYoNjY2HMfJyUlqXTp0mrWrJny5s17jd+BXSkpKYqPj1fTpk0VHh5uOw4QlLgOAe9z5MoljR59xfOiW7RQFWZMAV7B70PAPq5D+BLXarWssFqYioiIUPXq1bVkyRK1bt1akpSWlqYlS5aoZ8+emX5O3bp1tWTJEvXp0+fcsfj4eNWtWzfT8yMjIxUZGXnR8fDw8IC5WAPpewH8Fdch4EWNG0ulSkn79mXeZ8rhkEqVUljjxlJoqPfzAUGM34eAfVyH8AXZ+Rm0vitfbGysJk2apOnTp2vr1q3q3r27Tpw4oc6dO0uSOnbsqAEDBpw7v3fv3lq8eLHefPNNbdu2TUOGDNHGjRsvWcgCAAABJjRUGjPGjB2OjI+5Po6LoygFAADgB6wXptq3b69Ro0Zp0KBBio6OVkJCghYvXnyuwfnu3bt14MCBc+fXq1dPs2bN0sSJE1WlShXNnTtXn332mW6//XZb3wIAAPC2Nm2kuXOlkiUzHi9Vyhxv08ZOLgAAAGSLTzQ/79mz5yVnPC1fvvyiY+3atVO7du08nAoAAPi0Nm2kBx7Q2WXLlLBokaJbtGD5HgAAgJ/xicIUAADAVQkNlbNRI+07ccI0OqcoBQAA4FesL+UDAAAAAABAcKIwBQAAAAAAACsoTAEAAAAAAMAKClMAAAAAAACwgsIUAAAAAAAArKAwBQAAAAAAACsoTAEAAAAAAMAKClMAAAAAAACwgsIUAAAAAAAArKAwBQAAAAAAACsoTAEAAAAAAMAKClMAAAAAAACwgsIUAAAAAAAArKAwBQAAAAAAACsoTAEAAAAAAMCKMNsBvM3pdEqSkpKSLCe5dikpKTp58qSSkpIUHh5uOw4QlLgOAfu4DgH7uA4B+7gO4UtcNRdXDeZygq4wdezYMUlS6dKlLScBAAAAAAAIXMeOHVO+fPkue47DmZXyVQBJS0vT/v37lSdPHjkcDttxrklSUpJKly6tPXv2KG/evLbjAEGJ6xCwj+sQsI/rELCP6xC+xOl06tixYypRooRCQi7fRSroZkyFhISoVKlStmO4Vd68efmHB7CM6xCwj+sQsI/rELCP6xC+4kozpVxofg4AAAAAAAArKEwBAAAAAADACgpTfiwyMlKDBw9WZGSk7ShA0OI6BOzjOgTs4zoE7OM6hL8KuubnAAAAAAAA8A3MmAIAAAAAAIAVFKYAAAAAAABgBYUpAAAAAAAAWEFhyo+NGzdOZcqUUVRUlGrXrq3169fbjgQEjeHDh6tmzZrKkyePihQpotatW2v79u22YwFBa8SIEXI4HOrTp4/tKEDQ2bdvnx577DFdd911ypEjhypVqqSNGzfajgUEjdTUVA0cOFBly5ZVjhw5dNNNN2no0KGinTT8BYUpPzVnzhzFxsZq8ODB2rx5s6pUqaJ77rlHhw4dsh0NCArffPONevTooW+//Vbx8fFKSUlRs2bNdOLECdvRgKCzYcMGvffee6pcubLtKEDQ+fvvv1W/fn2Fh4dr0aJF+vnnn/Xmm2+qQIECtqMBQeP111/X+PHjNXbsWG3dulWvv/66Ro4cqXfeecd2NCBL2JXPT9WuXVs1a9bU2LFjJUlpaWkqXbq0nn32WfXv399yOiD4HD58WEWKFNE333yjO+64w3YcIGgcP35c1apV07vvvqtXX31V0dHRiouLsx0LCBr9+/fX6tWrtXLlSttRgKB13333qWjRopoyZcq5Yw8++KBy5MihGTNmWEwGZA0zpvzQmTNntGnTJjVp0uTcsZCQEDVp0kRr1661mAwIXkePHpUkFSxY0HISILj06NFDLVu2zPA7EYD3fPHFF6pRo4batWunIkWKqGrVqpo0aZLtWEBQqVevnpYsWaJffvlFkvT9999r1apVatGiheVkQNaE2Q6A7Dty5IhSU1NVtGjRDMeLFi2qbdu2WUoFBK+0tDT16dNH9evX1+233247DhA0Zs+erc2bN2vDhg22owBB67ffftP48eMVGxurl156SRs2bFCvXr0UERGhJ554wnY8ICj0799fSUlJqlChgkJDQ5WamqrXXntNjz76qO1oQJZQmAKAa9SjRw9t2bJFq1atsh0FCBp79uxR7969FR8fr6ioKNtxgKCVlpamGjVqaNiwYZKkqlWrasuWLZowYQKFKcBLPv74Y82cOVOzZs3SbbfdpoSEBPXp00clSpTgOoRfoDDlhwoVKqTQ0FAlJiZmOJ6YmKhixYpZSgUEp549e2rBggVasWKFSpUqZTsOEDQ2bdqkQ4cOqVq1aueOpaamasWKFRo7dqySk5MVGhpqMSEQHIoXL66KFStmOHbrrbfq008/tZQICD4vvPCC+vfvr4cffliSVKlSJe3atUvDhw+nMAW/QI8pPxQREaHq1atryZIl546lpaVpyZIlqlu3rsVkQPBwOp3q2bOn5s+fr6VLl6ps2bK2IwFB5e6779aPP/6ohISEc7caNWro0UcfVUJCAkUpwEvq16+v7du3Zzj2yy+/6IYbbrCUCAg+J0+eVEhIxj/tQ0NDlZaWZikRkD3MmPJTsbGxeuKJJ1SjRg3VqlVLcXFxOnHihDp37mw7GhAUevTooVmzZunzzz9Xnjx5dPDgQUlSvnz5lCNHDsvpgMCXJ0+ei3q65cqVS9dddx293gAveu6551SvXj0NGzZMDz30kNavX6+JEydq4sSJtqMBQaNVq1Z67bXXdP311+u2227Td999p9GjR+vJJ5+0HQ3IEofT6XTaDoGrM3bsWL3xxhs6ePCgoqOj9fbbb6t27dq2YwFBweFwZHp82rRp6tSpk3fDAJAk3XnnnYqOjlZcXJztKEBQWbBggQYMGKBff/1VZcuWVWxsrGJiYmzHAoLGsWPHNHDgQM2fP1+HDh1SiRIl1KFDBw0aNEgRERG24wFXRGEKAAAAAAAAVtBjCgAAAAAAAFZQmAIAAAAAAIAVFKYAAAAAAABgBYUpAAAAAAAAWEFhCgAAAAAAAFZQmAIAAAAAAIAVFKYAAAAAAABgBYUpAAAAAAAAWEFhCgAAwI/ceeed6tOnz2XPKVOmjOLi4rySBwAA4FpQmAIAAPCyTp06yeFwXHTbsWOH7WgAAABeFWY7AAAAQDBq3ry5pk2bluFY4cKFLaUBAACwgxlTAAAAFkRGRqpYsWIZbqGhofrmm29Uq1YtRUZGqnjx4urfv7/Onj17yec5dOiQWrVqpRw5cqhs2bKaOXOmF78LAACAa8OMKQAAAB+xb98+3XvvverUqZM++OADbdu2TTExMYqKitKQIUMy/ZxOnTpp//79WrZsmcLDw9WrVy8dOnTIu8EBAACuEoUpAAAACxYsWKDcuXOf+7hFixa65ZZbVLp0aY0dO1YOh0MVKlTQ/v371a9fPw0aNEghIRknu//yyy9atGiR1q9fr5o1a0qSpkyZoltvvdWr3wsAAMDVojAFAABgQePGjTV+/PhzH+fKlUs9evRQ3bp15XA4zh2vX7++jh8/rr179+r666/P8Bxbt25VWFiYqlevfu5YhQoVlD9/fo/nBwAAcAcKUwAAABbkypVLN998s+0YAAAAVtH8HAAAwEfceuutWrt2rZxO57ljq1evVp48eVSqVKmLzq9QoYLOnj2rTZs2nTu2fft2/fPPP96ICwAAcM0oTAEAAPiIZ555Rnv27NGzzz6rbdu26fPPP9fgwYMVGxt7UX8pSSpfvryaN2+ubt26ad26ddq0aZO6du2qHDlyWEgPAACQfRSmAAAAfETJkiW1cOFCrV+/XlWqVNHTTz+tLl266OWXX77k50ybNk0lSpRQo0aN1KZNGz311FMqUqSIF1MDAABcPYfz/LniAAAAAAAAgJcwYwoAAAAAAABWUJgCAAAAAACAFRSmAAAAAAAAYAWFKQAAAAAAAFhBYQoAAAAAAABWUJgCAAAAAACAFRSmAAAAAAAAYAWFKQAAAAAAAFhBYQoAAAAAAABWUJgCAAAAAACAFRSmAAAAAAAAYAWFKQAAAAAAAFjxf4DTJQNAKVyOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Accuracy: 0.51667\n",
      "Mean Testing Accuracy: 0.2875\n",
      "Mean Training F-Measure Score: 0.51418\n",
      "Mean Testing F-Measure Score: 0.22956\n"
     ]
    }
   ],
   "source": [
    "#Visualize and compare accuracy for different K values\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "plt.plot(cv_results['train_accuracy'], label='Training Score', color='blue', marker='o')\n",
    "plt.plot(cv_results['test_accuracy'], label='Testing Score', color='red', marker='o')\n",
    "\n",
    "plt.title('Model Accuracy Across 10-Fold Cross-Validation')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Visualize and compare accuracy for different F scores\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "plt.plot(cv_results['train_f1_weighted'], label='Training F-Measure', color='blue', marker='o')\n",
    "plt.plot(cv_results['test_f1_weighted'], label='Testing F-Measure', color='red', marker='o')\n",
    "\n",
    "plt.title('Model F-Measure Score Across 10-Fold Cross-Validation')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print average accuracy scores\n",
    "print(\"Mean Training Accuracy:\", round(np.mean(train_accuracy), 5))\n",
    "print(\"Mean Testing Accuracy:\", round(np.mean(test_accuracy), 5))\n",
    "print(\"Mean Training F-Measure Score:\", round(np.mean(train_f1), 5))\n",
    "print(\"Mean Testing F-Measure Score:\", round(np.mean(test_f1), 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
